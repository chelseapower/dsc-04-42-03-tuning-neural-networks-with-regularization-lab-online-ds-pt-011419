{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate randomm sample using seed 123\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "#Alternative custom script:\n",
    "# random.seed(123)\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9479 - acc: 0.1493 - val_loss: 1.9434 - val_acc: 0.1460\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9216 - acc: 0.1872 - val_loss: 1.9249 - val_acc: 0.1780\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9023 - acc: 0.2221 - val_loss: 1.9076 - val_acc: 0.1890\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8828 - acc: 0.2439 - val_loss: 1.8874 - val_acc: 0.2080\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8610 - acc: 0.2692 - val_loss: 1.8648 - val_acc: 0.2390\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8356 - acc: 0.2936 - val_loss: 1.8384 - val_acc: 0.2560\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8061 - acc: 0.3159 - val_loss: 1.8072 - val_acc: 0.2840\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7720 - acc: 0.3448 - val_loss: 1.7709 - val_acc: 0.3190\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7328 - acc: 0.3755 - val_loss: 1.7283 - val_acc: 0.3410\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6880 - acc: 0.4001 - val_loss: 1.6809 - val_acc: 0.3820\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6385 - acc: 0.4244 - val_loss: 1.6318 - val_acc: 0.4010\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5863 - acc: 0.4508 - val_loss: 1.5790 - val_acc: 0.4320\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5316 - acc: 0.4804 - val_loss: 1.5251 - val_acc: 0.4640\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4764 - acc: 0.5157 - val_loss: 1.4719 - val_acc: 0.4910\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4209 - acc: 0.5423 - val_loss: 1.4194 - val_acc: 0.5120\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3665 - acc: 0.5655 - val_loss: 1.3670 - val_acc: 0.5540\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3139 - acc: 0.5903 - val_loss: 1.3173 - val_acc: 0.5760\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2629 - acc: 0.6124 - val_loss: 1.2678 - val_acc: 0.5940\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2137 - acc: 0.6283 - val_loss: 1.2219 - val_acc: 0.6210\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1669 - acc: 0.6463 - val_loss: 1.1787 - val_acc: 0.6310\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1229 - acc: 0.6595 - val_loss: 1.1374 - val_acc: 0.6490\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0815 - acc: 0.6683 - val_loss: 1.0992 - val_acc: 0.6570\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0431 - acc: 0.6779 - val_loss: 1.0635 - val_acc: 0.6730\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0075 - acc: 0.6859 - val_loss: 1.0309 - val_acc: 0.6790\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9742 - acc: 0.6939 - val_loss: 1.0004 - val_acc: 0.6820\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9437 - acc: 0.7007 - val_loss: 0.9725 - val_acc: 0.6910\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9154 - acc: 0.7063 - val_loss: 0.9481 - val_acc: 0.7050\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8897 - acc: 0.7123 - val_loss: 0.9249 - val_acc: 0.7050\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8656 - acc: 0.7161 - val_loss: 0.9052 - val_acc: 0.7040\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8441 - acc: 0.7225 - val_loss: 0.8867 - val_acc: 0.7060\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.7252 - val_loss: 0.8728 - val_acc: 0.7040\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8056 - acc: 0.7287 - val_loss: 0.8515 - val_acc: 0.7160\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7878 - acc: 0.7339 - val_loss: 0.8404 - val_acc: 0.7180\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7722 - acc: 0.7371 - val_loss: 0.8237 - val_acc: 0.7200\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7565 - acc: 0.7424 - val_loss: 0.8113 - val_acc: 0.7200\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7430 - acc: 0.7445 - val_loss: 0.7992 - val_acc: 0.7260\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7299 - acc: 0.7459 - val_loss: 0.7889 - val_acc: 0.7290\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7175 - acc: 0.7525 - val_loss: 0.7805 - val_acc: 0.7250\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7062 - acc: 0.7556 - val_loss: 0.7704 - val_acc: 0.7270\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6954 - acc: 0.7553 - val_loss: 0.7618 - val_acc: 0.7310\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6849 - acc: 0.7591 - val_loss: 0.7544 - val_acc: 0.7310\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6752 - acc: 0.7635 - val_loss: 0.7490 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6659 - acc: 0.7653 - val_loss: 0.7408 - val_acc: 0.7310\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6574 - acc: 0.7672 - val_loss: 0.7350 - val_acc: 0.7330\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6490 - acc: 0.7695 - val_loss: 0.7300 - val_acc: 0.7350\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6409 - acc: 0.7721 - val_loss: 0.7252 - val_acc: 0.7360\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6333 - acc: 0.7755 - val_loss: 0.7185 - val_acc: 0.7420\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6258 - acc: 0.7777 - val_loss: 0.7132 - val_acc: 0.7370\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6185 - acc: 0.7799 - val_loss: 0.7086 - val_acc: 0.7340\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6110 - acc: 0.7828 - val_loss: 0.7059 - val_acc: 0.7360\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6051 - acc: 0.7825 - val_loss: 0.6999 - val_acc: 0.7400\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5986 - acc: 0.7835 - val_loss: 0.6973 - val_acc: 0.7420\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5928 - acc: 0.7872 - val_loss: 0.6929 - val_acc: 0.7380\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5866 - acc: 0.7901 - val_loss: 0.6900 - val_acc: 0.7470\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5811 - acc: 0.7903 - val_loss: 0.6879 - val_acc: 0.7500\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5750 - acc: 0.7949 - val_loss: 0.6833 - val_acc: 0.7460\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5697 - acc: 0.7940 - val_loss: 0.6815 - val_acc: 0.7410\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5644 - acc: 0.7963 - val_loss: 0.6806 - val_acc: 0.7450\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5591 - acc: 0.7985 - val_loss: 0.6782 - val_acc: 0.7500\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5540 - acc: 0.8037 - val_loss: 0.6738 - val_acc: 0.7460\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5494 - acc: 0.8028 - val_loss: 0.6703 - val_acc: 0.7500\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5441 - acc: 0.8049 - val_loss: 0.6708 - val_acc: 0.7530\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5395 - acc: 0.8056 - val_loss: 0.6668 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5347 - acc: 0.8085 - val_loss: 0.6665 - val_acc: 0.7520\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5301 - acc: 0.8104 - val_loss: 0.6634 - val_acc: 0.7460\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5256 - acc: 0.8107 - val_loss: 0.6605 - val_acc: 0.7520\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5214 - acc: 0.8120 - val_loss: 0.6596 - val_acc: 0.7590\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5169 - acc: 0.8149 - val_loss: 0.6579 - val_acc: 0.7470\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5128 - acc: 0.8165 - val_loss: 0.6558 - val_acc: 0.7560\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5086 - acc: 0.8189 - val_loss: 0.6595 - val_acc: 0.7570\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5050 - acc: 0.8191 - val_loss: 0.6534 - val_acc: 0.7610\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5003 - acc: 0.8216 - val_loss: 0.6536 - val_acc: 0.7610\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4967 - acc: 0.8227 - val_loss: 0.6515 - val_acc: 0.7550\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4929 - acc: 0.8236 - val_loss: 0.6499 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4890 - acc: 0.8273 - val_loss: 0.6489 - val_acc: 0.7560\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4849 - acc: 0.8272 - val_loss: 0.6479 - val_acc: 0.7570\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4814 - acc: 0.8275 - val_loss: 0.6533 - val_acc: 0.7550\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4781 - acc: 0.8269 - val_loss: 0.6469 - val_acc: 0.7550\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4740 - acc: 0.8299 - val_loss: 0.6462 - val_acc: 0.7540\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4705 - acc: 0.8301 - val_loss: 0.6467 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4671 - acc: 0.8317 - val_loss: 0.6446 - val_acc: 0.7580\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4633 - acc: 0.8345 - val_loss: 0.6449 - val_acc: 0.7580\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4599 - acc: 0.8368 - val_loss: 0.6423 - val_acc: 0.7560\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4566 - acc: 0.8347 - val_loss: 0.6407 - val_acc: 0.7610\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4531 - acc: 0.8383 - val_loss: 0.6452 - val_acc: 0.7540\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4505 - acc: 0.8409 - val_loss: 0.6441 - val_acc: 0.7620\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4468 - acc: 0.8415 - val_loss: 0.6401 - val_acc: 0.7570\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4431 - acc: 0.8425 - val_loss: 0.6428 - val_acc: 0.7590\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4402 - acc: 0.8444 - val_loss: 0.6403 - val_acc: 0.7600\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4373 - acc: 0.8456 - val_loss: 0.6414 - val_acc: 0.7690\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4344 - acc: 0.8464 - val_loss: 0.6379 - val_acc: 0.7610\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4308 - acc: 0.8485 - val_loss: 0.6405 - val_acc: 0.7580\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4281 - acc: 0.8484 - val_loss: 0.6386 - val_acc: 0.7670\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4252 - acc: 0.8511 - val_loss: 0.6367 - val_acc: 0.7630\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4219 - acc: 0.8525 - val_loss: 0.6383 - val_acc: 0.7590\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4186 - acc: 0.8536 - val_loss: 0.6430 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4161 - acc: 0.8544 - val_loss: 0.6390 - val_acc: 0.7580\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4129 - acc: 0.8557 - val_loss: 0.6393 - val_acc: 0.7600\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4103 - acc: 0.8571 - val_loss: 0.6367 - val_acc: 0.7620\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4071 - acc: 0.8593 - val_loss: 0.6369 - val_acc: 0.7580\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4047 - acc: 0.8604 - val_loss: 0.6375 - val_acc: 0.7620\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4020 - acc: 0.8615 - val_loss: 0.6393 - val_acc: 0.7620\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3993 - acc: 0.8624 - val_loss: 0.6364 - val_acc: 0.7600\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3963 - acc: 0.8640 - val_loss: 0.6370 - val_acc: 0.7640\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3934 - acc: 0.8651 - val_loss: 0.6385 - val_acc: 0.7720\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3907 - acc: 0.8671 - val_loss: 0.6365 - val_acc: 0.7620\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3877 - acc: 0.8677 - val_loss: 0.6391 - val_acc: 0.7690\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3852 - acc: 0.8680 - val_loss: 0.6365 - val_acc: 0.7640\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3829 - acc: 0.8695 - val_loss: 0.6371 - val_acc: 0.7640\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3800 - acc: 0.8705 - val_loss: 0.6371 - val_acc: 0.7670\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3774 - acc: 0.8697 - val_loss: 0.6367 - val_acc: 0.7650\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3749 - acc: 0.8724 - val_loss: 0.6393 - val_acc: 0.7700\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3724 - acc: 0.8747 - val_loss: 0.6378 - val_acc: 0.7630\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3694 - acc: 0.8763 - val_loss: 0.6418 - val_acc: 0.7680\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3677 - acc: 0.8764 - val_loss: 0.6394 - val_acc: 0.7710\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3652 - acc: 0.8785 - val_loss: 0.6400 - val_acc: 0.7640\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3623 - acc: 0.8776 - val_loss: 0.6418 - val_acc: 0.7690\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3602 - acc: 0.8796 - val_loss: 0.6416 - val_acc: 0.7630\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3575 - acc: 0.8821 - val_loss: 0.6439 - val_acc: 0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3554 - acc: 0.8815 - val_loss: 0.6395 - val_acc: 0.7700\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35173031256198883, 0.8826666666666667]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5740682258605957, 0.7866666671435039]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4FFXWwOHfIexr2EV2BBWIrBEUUEEWQVkUQURQcUMcBPcRPxlldBwdHRUURBFERxFQFllkV5RNVkWQTSJrSMAQICBLIOR8f9xKaEICSUjT6eS8z9MPXdVV1ae6Qp2qe2/dK6qKMcYYA5An0AEYY4zJPiwpGGOMSWZJwRhjTDJLCsYYY5JZUjDGGJPMkoIxxphklhRMuolIiIj8JSJVsnLZ7E5EvhCRId77liKyIT3LZuJ7csxvZoKXJYUczDvBJL0SReS4z3SvjG5PVU+ralFV3ZWVy2aGiFwrIj+LyBER2SwibfzxPSmp6g+qWjcrtiUiS0Skj8+2/fqbGZMelhRyMO8EU1RViwK7gE4+88alXF5E8l76KDPtA2A6UBy4FdgT2HBMWkQkj4jYuSZI2IHKxUTkXyIyUUTGi8gRoLeIXC8iy0XkkIhEi8h7IpLPWz6viKiIVPOmv/A+n+1dsf8kItUzuqz3eQcR+V1E4kTkfRFZ6nsVnYoEYKc621R10wX2dauItPeZzi8iB0SknnfSmiQie739/kFEaqexnTYissNnurGIrPX2aTxQwOez0iIyS0RiROSgiMwQkYreZ/8Brgc+9O7chqbym4V6v1uMiOwQkRdERLzPHhaRH0XkXS/mbSLS7jz7P9hb5oiIbBCRzik+f9S74zoiIr+JSH1vflUR+caLYb+IDPPm/0tEPvVZv6aIqM/0EhF5VUR+Ao4CVbyYN3nf8YeIPJwihq7eb3lYRCJEpJ2I9BSRFSmWe15EJqW1r+biWFIwdwBfAiWAibiT7RNAGaA50B549Dzr3wP8AyiFuxt5NaPLikg54CvgOe97twNNLhD3SuDtpJNXOowHevpMdwCiVHWdNz0TqAVcBvwGfH6hDYpIAWAa8Alun6YBt/sskgf4GKgCVAVOAcMAVPV54Cegn3fn9mQqX/EBUBioAdwMPATc5/N5M2A9UBp4FxhznnB/xx3PEsBrwJciUt7bj57AYKAX7s6rK3DAu3P8FogAqgGVcccpve4FHvS2GQnsA27zph8B3heRel4MzXC/4zNAKNAK2Al8A1wlIrV8ttubdBwfk0mqaq9c8AJ2AG1SzPsX8P0F1nsW+Np7nxdQoJo3/QXwoc+ynYHfMrHsg8Bin88EiAb6pBFTb2A1rtgoEqjnze8ArEhjnauBOKCgNz0R+L80li3jxV7EJ/Yh3vs2wA7v/c3AbkB81l2ZtGwq2w0HYnyml/juo+9vBuTDJegrfT7vDyzw3j8MbPb5rLi3bpl0/j38Btzmvf8O6J/KMjcAe4GQVD77F/Cpz3RNdzo5a99eukAMM5O+F5fQ3kpjuY+Bf3rvGwD7gXyB/j+VU192p2B2+06IyNUi8q1XlHIYeAV3kkzLXp/3x4CimVj2ct841P3vjzzPdp4A3lPVWbgT5TzvirMZsCC1FVR1M/AHcJuIFAU64u6Qklr9vOkVrxzGXRnD+fc7Ke5IL94kO5PeiEgRERktIru87X6fjm0mKQeE+G7Pe1/RZzrl7wlp/P4i0kdEfvWKmg7hkmRSLJVxv01KlXEJ8HQ6Y04p5d9WRxFZ4RXbHQLapSMGgM9wdzHgLggmquqpTMZkLsCSgknZTe5HuKvImqpaHHgJd+XuT9FApaQJr9y8YtqLkxd3FY2qTgOexyWD3sDQ86yXVIR0B7BWVXd48+/D3XXcjCteqZkUSkbi9vg2J/07UB1o4v2WN6dY9nxdFP8JnMYVO/luO8MV6iJSAxgJPAaUVtVQYDNn9m83cEUqq+4GqopISCqfHcUVbSW5LJVlfOsYCgGTgNeB8l4M89IRA6q6xNtGc9zxs6IjP7KkYFIqhitmOepVtp6vPiGrzAQaiUgnrxz7CaDseZb/GhgiIteIa9WyGTgJFAIKnme98bgipr54dwmeYkA8EIs70b2WzriXAHlE5HGvkrg70CjFdo8BB0WkNC7B+tqHqy84h3clPAn4t4gUFVcp/xSuKCujiuJO0DG4nPsw7k4hyWjg7yLSUJxaIlIZV+cR68VQWEQKeSdmgLXATSJSWURCgUEXiKEAkN+L4bSIdARa+3w+BnhYRFqJq/ivJCJX+Xz+OS6xHVXV5Zn4DUw6WVIwKT0D3A8cwd01TPT3F6rqPqAH8A7uJHQF8AvuRJ2a/wD/wzVJPYC7O3gYd9L/VkSKp/E9kbi6iOs4u8J0LBDlvTYAy9IZdzzuruMR4CCugvYbn0Xewd15xHrbnJ1iE0OBnl6RzjupfMXfcMluO/Ajrhjlf+mJLUWc64D3cPUd0biEsMLn8/G433QicBiYApRU1QRcMVtt3JX8LqCbt9ocYCquonsl7licL4ZDuKQ2FXfMuuEuBpI+X4b7Hd/DXZQsxBUpJfkfEIbdJfidnF0cakzgecUVUUA3VV0c6HhM4IlIEVyRWpiqbg90PDmZ3SmYbEFE2otICa+Z5z9wdQYrAxyWyT76A0stIfhfMD3BanK2FsA4XLnzBuB2r3jG5HIiEol7xqNLoGPJDaz4yBhjTDIrPjLGGJMs6IqPypQpo9WqVQt0GMYYE1TWrFmzX1XP19QbCMKkUK1aNVavXh3oMIwxJqiIyM4LL+Xn4iOvRckWr8fDcx5u8Xpg/E5E1onrmTLl06HGGGMuIb8lBa+t+QjcE6R1cA/p1Emx2H+B/6lqPVwfO6/7Kx5jjDEX5s87hSZAhLq+7k8CEzi3SVkdXA+N4J5gtCZnxhgTQP6sU6jI2b0kRgJNUyzzK3Anro/5O4BiIlJaVWN9FxKRvrj+aqhS5dzha0+dOkVkZCQnTpzIuuhNlitYsCCVKlUiX758gQ7FGJMGfyaF1HqYTPlQxLPAcHEjbC3C9QCZcM5KqqOAUQDh4eHnPFgRGRlJsWLFqFatGq6DTZPdqCqxsbFERkZSvXr1C69gjAkIfyaFSM7u0KoSrj+bZKoahetEDK+P+ztVNS6jX3TixAlLCNmciFC6dGliYmICHYox5jz8WaewCqglItVFJD9wNyl6UhSRMnJmQO8XcMPxZYolhOzPjpEx2Z/f7hRUNUFEHgfm4kaQ+kRVN4jIK8BqVZ0OtARe9wb8XoTr9MoYY0xkJCxYAHv2QMmSUKoUNGkCNVIdgiPL+PXhNW+4xFkp5r3k834SbiCRoBYbG0vr1m68kL179xISEkLZsu7BwZUrV5I/f/4LbuOBBx5g0KBBXHXVVWkuM2LECEJDQ+nVq1eayxhjgoAq7NgBy5bB5s3uhF+hAsTGwqpV8NNP8Pvv56734YfwqH/HvQq6J5qzo9KlS7N27VoAhgwZQtGiRXn22WfPWiZ5UOw8qZfYjR079oLf07+/3UgZEzROnoTVq2HJEli3DjZsgJ074dQp94pPoxPgcuWgaVPo1w/atIErr4RDh+DAASh7wV4qLpolBT+KiIjg9ttvp0WLFqxYsYKZM2fyz3/+k59//pnjx4/To0cPXnrJ3Ti1aNGC4cOHExYWRpkyZejXrx+zZ8+mcOHCTJs2jXLlyjF48GDKlCnDk08+SYsWLWjRogXff/89cXFxjB07lmbNmnH06FHuu+8+IiIiqFOnDlu3bmX06NE0aNDgrNhefvllZs2axfHjx2nRogUjR45ERPj999/p168fsbGxhISEMGXKFKpVq8a///1vxo8fT548eejYsSOvvZbeESuNyYFUISoKdu2C3bvdCb5cOShWzF3lz50LixdDUjP5ypWhbl1o3hwKFICQEKheHa6/HsLC4PBhiI5261euDCnr38qXd69LIOclhSefBO+qPcs0aABDzzcefNo2btzI2LFj+fDDDwF44403KFWqFAkJCbRq1Ypu3bpRp87ZD3rHxcVx00038cYbb/D000/zySefMGjQuUPgqiorV65k+vTpvPLKK8yZM4f333+fyy67jMmTJ/Prr7/SqFGjc9YDeOKJJ/jnP/+JqnLPPfcwZ84cOnToQM+ePRkyZAidOnXixIkTJCYmMmPGDGbPns3KlSspVKgQBw4cyNRvYUxQOHUK1q93xTh79rginUOH3Ak+Pt4lgy1b4NixtLdRt6670r/xRmjR4sJX+KVKuVc2kPOSQjZzxRVXcO211yZPjx8/njFjxpCQkEBUVBQbN248JykUKlSIDh06ANC4cWMWL059RMquXbsmL7Njxw4AlixZwvPPPw9A/fr1qVu3bqrrfvfdd7z11lucOHGC/fv307hxY6677jr2799Pp06dAPewGcCCBQt48MEHKVSoEAClsskfrzGZduSIuzLfvt0V62za5Mr4d+92/yYV7Yi4St7QUChUyF3lly8PN93kinWqVnVX9gUKQEwMHDwI9eu7eUEq5yWFTF7R+0uRIkWS32/dupVhw4axcuVKQkND6d27d6pPYftWTIeEhJCQcM7zfAAUKFDgnGXSM2jSsWPHePzxx/n555+pWLEigwcPTo4jtWajqmrNSU1wSUyEjRth5UpXjh8Z6U74u3e793/9dfbyZcu6Vj3XXAOdO0N4OFx7rTvpp1EPeI4rr8z6/QiAnJcUsrHDhw9TrFgxihcvTnR0NHPnzqV9+/ZZ+h0tWrTgq6++4oYbbmD9+vVs3LjxnGWOHz9Onjx5KFOmDEeOHGHy5Mn06tWLkiVLUqZMGWbMmHFW8VG7du34z3/+Q48ePZKLj+xuwWQLx465K/s//oCIiDOv1atdxSy4q/0KFaBSJVesc8stcPnl7lW5MtSpc0kqcIOFJYVLqFGjRtSpU4ewsDBq1KhB8+bNs/w7BgwYwH333Ue9evVo1KgRYWFhlChR4qxlSpcuzf33309YWBhVq1aladMzXVKNGzeORx99lBdffJH8+fMzefJkOnbsyK+//kp4eDj58uWjU6dOvPrqq1keuzHnOHrU1RFu2eKaaO7c6a72o6Lgzz/d575CQ6FmTbj9drjhBmjWzFXoWn9b6RZ0YzSHh4drykF2Nm3aRO3atQMUUfaSkJBAQkICBQsWZOvWrbRr146tW7eSN2/2yP92rMw5EhPdSf+nn1w5f968rjnnokXudfKkWy5fPqhSxV3dV6zoWvuULeuKeGrWhCuugNKlA7sv2ZiIrFHV8Astlz3OFCbL/PXXX7Ru3ZqEhARUlY8++ijbJASTix075trqr1/vKnd37HBX+zExroz/8OFz16lbFwYMcJW6deq4k7/9Lfud/cI5TGhoKGvWrAl0GCY3ioiA77+HvXvdST429kzlbkSEuyMAd2KvUsWV6deuDa1aQePGZ4p6Tp92zwEULhzY/cmlLCkYY9InPt5V4CYkuNY5RYq4Fj5jx8LUqa6yN0mRIq4pZ6VKUK8e9OjhTvxJzTVDQgK3H+a8LCkYY1Kn6op7Zs50T+iuWHGm/X5IiDu579jhrvzbtoWnnoJ27dzVvhXzBC07csbkdrt2wezZrtgnPt49gLVpk3uoa/9+t0yjRtC/v2vRky+fqxT+7TcYOBB69XKVviZHsKRgTE504AB8/jlMmeJa74SEuCv/+Hg3XagQFC/uTvq+3cLkzev637n6ates8/rroUMH187f1223Xdr9MZeMPwfZyTVatmzJ3Llzz5o3dOhQ/va3v513vaJFiwIQFRVFt27d0tx2yia4KQ0dOpRjPv2w3HrrrRw6dCg9oZtgpuqu1idNgnffhWeegXvugZYtXSXuk0+6Ct/ixaFgQZcILrvMNd8sWdI91RsaCv/5j7szSEhw/f4cOOC6dP74Y3jwwXMTgsnR7E4hC/Ts2ZMJEyZwyy23JM+bMGECb731VrrWv/zyy5k0KfPDSgwdOpTevXtT2GutMWvWrAusYYJKfLzrrmHxYnfCTkiAfftcS58//zyzXKFC7gR++eWuz/0HH3QVu8ZkgN0pZIFu3boxc+ZM4r1KuB07dhAVFUWLFi2Snxto1KgR11xzDdOmTTtn/R07dhAWFga4Lijuvvtu6tWrR48ePTh+/Hjyco899hjh4eHUrVuXl19+GYD33nuPqKgoWrVqRatWrQCoVq0a+72y4HfeeYewsDDCwsIY6vULtWPHDmrXrs0jjzxC3bp1adeu3Vnfk2TGjBk0bdqUhg0b0qZNG/bt2we4ZyEeeOABrrnmGurVq8fkyZMBmDNnDo0aNaJ+/frJgw6ZTNi6FYYNcyf1pk3dVf2NN8KLL7pBVsaOdX30t23r3q9d65p/Hj3qWgAtXuzWt4RgMiHH3SkEoufs0qVL06RJE+bMmUOXLl2YMGECPXr0QEQoWLAgU6dOpXjx4uzfv5/rrruOzp07p9nB3MiRIylcuDDr1q1j3bp1Z3V9/dprr1GqVClOnz5N69atWbduHQMHDuSdd95h4cKFlClT5qxtrVmzhrFjx7JixQpUlaZNm3LTTTdRsmRJtm7dyvjx4/n444+56667mDx5Mr179z5r/RYtWrB8+XJEhNGjR/Pmm2/y9ttv8+qrr1KiRAnWr18PwMGDB4mJieGRRx5h0aJFVK9e3brXPp/Tp90TvNHR7i7g6FHXiiciwp3sk/qrKl/+TBfMN93kKnmtzynjZ35NCiLSHhiGG6N5tKq+keLzKsBnQKi3zCBvCM+gk1SElJQUPvnkE8D1MPp///d/LFq0iDx58rBnzx727dvHZZddlup2Fi1axMCBAwGoV68e9erVS/7sq6++YtSoUSQkJBAdHc3GjRvP+jylJUuWcMcddyT31Nq1a1cWL15M586dqV69evLAO75db/uKjIykR48eREdHc/LkSapXrw64rrQnTJiQvFzJkiWZMWMGN954Y/Iy1mGeD1X49VeYNs017Vy7FlK5M6NMGXd1/+ijrqfOatUueajG+C0piEgIMAJoC0QCq0Rkuqr6dts5GPhKVUeKSB3ceM7VLuZ7A9Vz9u23387TTz+dPKpa0hX+uHHjiImJYc2aNeTLl49q1aql2l22r9TuIrZv385///tfVq1aRcmSJenTp88Ft3O+fq2Sut0G1/V2asVHAwYM4Omnn6Zz58788MMPDBkyJHm7KWPM1d1rr10LS5e6Zpnlyrnin2XLXCVwbKxr4XP4sOuts2lTd9Jv1Mh121CggKsLqFoVUnRcaEwg+PNOoQkQoarbAERkAtAF8E0KChT33pcAovwYj18VLVqUli1b8uCDD9KzZ8/k+XFxcZQrV458+fKxcOFCdu7ced7t3HjjjYwbN45WrVrx22+/sW7dOsB1u12kSBFKlCjBvn37mD17Ni1btgSgWLFiHDly5JzioxtvvJE+ffowaNAgVJWpU6fy+eefp3uf4uLiqFixIgCfffZZ8vx27doxfPjw5DqKgwcPcv3119O/f3+2b9+eXHyU4+8Wdu6EwYPhiy/O/ax0aXfiv/JKVyfQoAF07HjJhlQ0JrP8mRQqArt9piOBpimWGQLME5EBQBGgTWobEpG+QF+AKlWqZHmgWaVnz5507dr1rKKVXr160alTJ8LDw2nQoAFXX331ebfx2GOP8cADD1CvXj0aNGhAkyZNADeKWsOGDalbt+453W737duXDh06UKFCBRYuXJg8v1GjRvTp0yd5Gw8//DANGzZMtagoNUOGDKF79+5UrFiR6667ju3btwMwePBg+vfvT1hYGCEhIbz88st07dqVUaNG0bVrVxITEylXrhzz589P1/dkW6dOuTb9SQMlRUe7Styk17p17kp/0CBX7h8X51oFVa0KtWqdO86uMUHAb11ni0h34BZVfdibvhdooqoDfJZ52ovhbRG5HhgDhKlqYlrbta6zg1tQHKsjR2DkSPjvf10vnsWKuVeUdyNbuLB7qOuGG1wLoSAeetHkHtmh6+xIwPd/SyXOLR56CGgPoKo/iUhBoAzwJ8ZcCjt3wvTp8O23rpuHhATXlXNcnOvHp1UrN//gQdex2w03QMOGNmiLybH8mRRWAbVEpDqwB7gbuCfFMruA1sCnIlIbKAjE+DEmY1zf/l9/DaNHuyag4Lp1uOoq1x1EkyauMrhpytJOY3I+vyUFVU0QkceBubjmpp+o6gYReQVYrarTgWeAj0XkKVylcx/NZHlWrm79EiQu2Sh/iYnuCeBp02DBAlc3UKCAuwuIjnZPAZ8+7SqBX38dunbNMYOuG3Ox/PqcgvfMwawU817yeb8RuOiBigsWLEhsbCylS5e2xJBNqSqxsbEULFgw6zd+7BgsX+6u+pctc108HzrkrvqbN3f9+8THu+kGDVw3EG3auKeE7e/FmLPkiCeaK1WqRGRkJDExVvKUnRUsWJBKlSplzcb273cdwU2Y4J4RSEhwJ/iwMLjrLvcEcIcOrjmoMSbdckRSyJcvX/KTtCaHOn3aPSQ2fz7MmXMmEdSuDc8+6676mzWzB8CCxIkTrpfu7DQWjyq8+ip884273qhR48xncXEZ+9PautUNU5GyC7CTJ+GHH9zwFeHhrlPbpJvVhATIk8e9wJV6Dhjg+j1s2NBVdXXs6Kq+/EpVg+rVuHFjNTlcYqLqzp2qU6eq/uMfqu3aqRYrpur+36o2bKj6wguqa9e6ZU1Q2bxZtXJl1bAw1e3bz/388GF32OfMUT19OvVtJCRkbUzHjqnedZf788qXT7VaNdXdu1Xj4lTvucfN79lT9fffz17vjz9U335bdfBg9+e6dq3qww+rhoS4dfr0Uf3rL9XYWNW//121RAk3P+nznj1V9+5V/e9/VUuXVq1Y0W3n+HHVzp3dMm3aqFap4t6PGpX5fcTV5V7wHBvwk3xGX5YUcqijR1VHjFBt29b970hKAHnyqF5zjepjj6l++aVqdHSgIzXnsWWLO8Gm5ZdfVMuWda/QUPfv0qVnPo+NVW3S5Mzhr1VL9d13VQ8edJ9v3656222qJUuqfvXVudtPSFDdsMGdaH1FR6v+8IN7LVqk+uefbv7Jk6oTJ6rWr68qovrmm6qrVrlrkCuvVL3iCvcneNddqoULu5N5eLiL8aqrzsSZdJIH1fz5VQcOdNctIqo1aqgWL+7e9+ihOn26SxT/+pfbtohbr21b96cOLjmA+y+RZO9e1UOHMnVYVNWSggkWe/aoDhmiWqaM+3OsW1f1oYfc/4bly12yyCV+/dWdsNJy+rS7eu7Y0Z148uZ1r1tuUd26NWtiSEhQ3bhRdf36s1+bN6ueOuWWOXhQ9cUXVcuXV331VbdOYqLqa6+dOaF99JFLDr/8ovrxx+4Eef/97kq5cmWXPDZvVq1Z012Zd+zo1gkLUy1QQHXSJNVx41Svv95ts3Bh1e7d3b9Fipw5efbt65LDs8+q3nij+wzc73LXXarjx7t/8+Y9c9JOelWrplqhgntfvbrqtGlnfofFi1ULFXKxLl7s5u3dq/rMM6rt27tXp06q77yjum2b29dly9y+7thxZjvffee23aWL6rp15/7eixer9uqlunChmz55UvWNN1xcn32WNcc0SXqTgt+eaPaX1J5oNkEgIcF1DfHDD266QAHXYujbb10T0o4d4e9/hxYtcl2LoMREN3DaoEFueuFC9zOA+7mGD3fP0+3Y4XrRKFcOevRwD1kfPw5jxriy6ueec4Oqqbr+9/bsOfsVEuLKscPDXYMsgPz5XWOsMmVg5kz3IHdavaAUKuS6c9q40T3L16CBq+a5+WY3ts+4ca51b3S0G8JZxMUCru6gQgVXBTRqlOsJBFx/gf/6lxs1dNcu16PItGlnl8X/8guMGAETJ7pnCYcPd9v6xz/coHFJ+9GggXu0JDzc9UAyZoxrhBYa6h4879DB/QYnT8L69a7V8qlT8PDD0L69+8zX7t1u3WLFLvIAZxPpfaLZkoLxr2PHYMgQNxjM/v1nnynKl4cHHoCHHnJnsxzgxAlXFz51qjt51q8P117rTlp79riWsf36uVExwc175BFX8XjHHa5j1SNHYM0a17L27rtd33p160LFiu4h6zvvdDk1SVQUDBwI3lhHyfLlcyf8ihXd68QJdyL0xkpKVcuWcO+9bgRPX8eOuZPzypUuKb30kjsJf/YZ9O/vPn/1VTcOEMCsWe4aIGn/q1c/96TrS73exUuXzlivIevXu9+0Xj33G6eMecUKlyi8QQlzNUsKJvDWrIHevWHzZtdM9K673CVZ4cLuci1fvjNNLQJs5053JTp1qjvRdO0Kjz/uWnykdPAgvPmmu+Jt1MgtV6sWfPSRO0nGxbmTar167iTvO1y2iGvF8uab7ir1hRfcT/H22/DYYy6RNG3qTubbtrnv//bb9LWsTUo6AEWLuqv/lD+vquu1I6mn9OPHXVKJjnb74g0AmCF//OG2ccMNGV/XXDrpTQoBryPI6MvqFLKpAwdUZ81yzUa6d3cthPLmdQXM8+cHOrrzWrDgTFl048aq992nWrSom27e3FVOqqpGRrrdK1HCVQ526qRaqdKZMup8+VxLlTlzVOPj3TqJia68//ffXeXi5s2qN910Zp02bVQjIs6O5+uv3WetW6seOXJJfwqTg2F1CuaSiIhwheFJZRd58rgG3jVrukvl55/32xCSERFu83nzuvLl813lbt/uysxXrnTlzY0auWKPqCjo3t31cvHNN3DFFW75I0fcVf/rr7tl6td3V/2JiW5QtFdecbuXkOD609u1C3r2TN9wCaowfryLu3v31KtQNm92P2PKIhFjMsuKj4z/qLoC5rFjXZlJ/vyuULttW1eAXLSo3772wAFXTDJjhqugTDppHjniHgR64AH3HFu+fK4YZ/FiN9b9rFlu/QoVXPJYtswNjQwu5DlzUs9dx4/DBx+4StTWrV0Rj+9DTcYEC0sKJuudOOHOkCNHusv0kBB3Fn7lFXe2zWJ79rgr+6TX6tWuVU2SO++E995zla5vvulapRw75srfy5WDLVvccuXLQ9++rgVK0rDHcXHuTmDzZteCJae0MDEmLZYUTNaJj4evvnJlNDt3uiYqvXq55jKlS6e6ypYtsGmTu3lIGrgMXKXq1Kkut2zYkPZXJiS4Eze4YpaLGnkuAAAd7ElEQVSkVixXXeVa0tSq5Vq/+Dp6FObNc8VAcXGuaWKTJi5cK4YxuV12GGTHBLOTJ10Zy8SJ7nXwoCuIHzPm3A5dPMePwzvvuCGLN29280qUcDcTlSq5q/0ff3RNImvUcI2R0mp8JOJO/E2auJN/ejpXLVLE5ak77sjkPhtjLCmYFH791T1XsGAB/PWXe2Lpjjtc4/V27dI8iy9Y4Nrf//GHe8Dob39zlbeffuqKdRIS3ANLLVvCffe5lqnZpDWqMcaHJQXjnDoFb7zh6gdCQ10SuOUWd1fgVRwnlfGvWuX+/flnV4YProSpVi3Xo2OrVmc2e8stLimcPu3K+Y0x2ZslhdwuLg4+/9z1I7B5s3uEdvjws+oKNm92vVD88YebTirjv+uuM90lXH65q8xNrZgnjWoHY0w2ZEkht4qPd3cGb77pLvfDw10HNCkK5A8fdrMOH4Zhw9zTtvXrp6+M3xgTfPyaFESkPTAMN0bzaFV9I8Xn7wJJhQ2FgXKqGurPmAywaJEbmD6p+4nnnnNJAdeCZ/t29xBXgQJw//1uwJAFC1x9gDEmZ/NbUhCREGAE0BaIBFaJyHR14zIDoKpP+Sw/AGjor3gMsGcPfz31Dzp8/QDrZAUULkTeBfmosME189y798xTu3nzuk7Mtm51PXhaQjAmd/DnnUITIEJVtwGIyASgC7AxjeV7Ai/7MZ7cS9XVE7zwAs8df4elNKffQ6cpUDQf8fGuM7Q9e1zPnV26uArjTZtcZXLHjvDEE4HeAWPMpeLPpFAR2O0zHQk0TW1BEakKVAe+T+PzvkBfgCpVqmRtlDndvn3uQYHZs5kb/iIfru7L00/D229be1BjzLn8eWZIbaSUtB6fvhuYpKqnU/tQVUepariqhpctWzbLAszxli1ztcILF3LozVE8FP0qtWvDa68FOjBjTHblz6QQCfgOl1EJiEpj2buB8X6MJff54gv3wECxYhz58We6zn6EvXuFzz6zlkPGmLT5MymsAmqJSHURyY878U9PuZCIXAWUBH7yYyy5R2KiG/7q3nuhWTMOzF5B24G1WbTIdQB37bWBDtAYk535rU5BVRNE5HFgLq5J6iequkFEXsEN9pCUIHoCEzTYeubLjo4edW1IJ09mZ4+/8821rzGyY162b3fDHXTpEugAjTHZnfWSmlPExECHDuxYE8vjdb7j242u0/+wMBg6NM0+7IwxuUR6e0m1Jig5QUwMenNr3l13M3ULbOWHnTV45RX3jMH69ZYQjDHpZ91cBLv9+6FNG/63pQlPn3qTjh1dN0bWctcYkxmWFIJZdDTccgtxv+/j70VGcN3VMG2adUltjMk8O30Eqz/+gObNYft2Xr5tFTFxBRgxwhKCMebi2J1CMIqIgBtugFOnWD/qJ4bfW4VHH3UDoxljzMWwpBBsTp50Yx7Ex7Prq+Xc89SVhIbaU8rGmKxhSSHYDB4Ma9aw4o2FdOl9JcePu2cQSpUKdGDGmJzAkkIwmT8f3nqLVXf8m5tebknFim74yzp1Ah2YMSansKQQLI4fhwcfRK+uzXP7/05oKCxfDtY/oDEmK1lblWDxwQcQGcm8B8bz4+IQBg+2hGCMyXrWzUUwOHwYatQgsfG1hMfM5uBB2LIF8ucPdGDGmGBh3VzkJG+/DbGxfN3qA375BV591RKCMcY/LClkd3/+CW+/TWTHfjz3QXXCwqBnz0AHZYzJqayiObt76SX2HS9O69+GcegQTJ0KISGBDsoYk1NZUsjOVq3i4Edf0a7Mb+zel59586Bx40AHZYzJySwpZFenT8Pf/sY7Rf/BbwcqMGcOtGgR6KCMMTmdJYXsaswYdPVqviz3PW2aCW3bBjogY0xuYBXN2dHRo/B//8eKho+x7c9i3HNPoAMyxuQWfk0KItJeRLaISISIDEpjmbtEZKOIbBCRL/0ZT9AYNw5iY/myxmAKFoQ77gh0QMaY3MJvxUciEgKMANoCkcAqEZmuqht9lqkFvAA0V9WDIlLOX/EEDVV47z0SGl7LxMUV6NQJihcPdFDGmNzCn3cKTYAIVd2mqieBCUCXFMs8AoxQ1YMAqvqnH+MJDt9/Dxs28N3Nr/Hnn2JFR8aYS8qfSaEisNtnOtKb5+tK4EoRWSoiy0WkfWobEpG+IrJaRFbHxMT4Kdxs4r33oGxZvtx7M6Gh0KFDoAMyxuQm/kwKksq8lB0t5QVqAS2BnsBoEQk9ZyXVUaoarqrhZXNyL3DbtsGMGcTe+yRTpoVw551QoECggzLG5Cb+TAqRQGWf6UpAVCrLTFPVU6q6HdiCSxK508iREBLC60cHcOwYPPVUoAMyxuQ2/kwKq4BaIlJdRPIDdwPTUyzzDdAKQETK4IqTtvkxpuzr1Cn47DMi2/Rh+KfFuPdeqFs30EEZY3IbvyUFVU0AHgfmApuAr1R1g4i8IiKdvcXmArEishFYCDynqrH+iilb+/ZbiInhn4n/IDERhgwJdEDGmNzIxlPILjp1YsuKQ9Q9sIj+/YVhwwIdkDEmJ0nveArWzUV2EBUFs2bxep2VFDgqvPhioAMyxuRW1s1FdvD55+xLLMP4LQ154AEoZ4/wGWMCxJJCoKnCJ5/wUZV/c/JUHgYMCHRAxpjczJJCoC1axMnftzMyrift28NVVwU6IGNMbmZJIdBGjuSrwg+wN64wTzwR6GCMMbmdVTQH0r596OQpDCsdwVWVoV27QAdkjMnt7E4hkMaMISKhKqv3VeGxxyCPHQ1jTIDZaShQTp+Gjz5iWZ2HAWjTJsDxGGMMlhQCZ/Zs2LWLZRW7U6IE1K4d6ICMMSadSUFErhCRAt77liIyMLXeTE0GjBoFFSqwLKo6119vRUfGmOwhvaeiycBpEakJjAGqAzZ0Zmbt3w+zZ3Oo28Ns2Cg0bx7ogIwxxklvUkj0Ori7Axiqqk8BFfwXVg739deQkMDy2n1QhWbNAh2QMcY46U0Kp0SkJ3A/MNObl88/IeUCX3wB11zDsuga5MkDTZoEOiBjjHHSmxQeAK4HXlPV7SJSHfjCf2HlYNu2wbJl0Ls3y5ZB/fpQtGiggzLGGCddSUFVN6rqQFUdLyIlgWKq+oafY8uZxo0DERK692TFCis6MsZkL+ltffSDiBQXkVLAr8BYEXnHv6HlQKqu6KhlS36Lq8xff1lSMMZkL+ktPiqhqoeBrsBYVW0M2ONWGbV2Lfz+O/TqxbJlbpYlBWNMdpLepJBXRCoAd3Gmotlk1Ny57t+OHZk/HypUgKpVAxuSMcb4Sm9SeAU3nvIfqrpKRGoAWy+0koi0F5EtIhIhIoNS+byPiMSIyFrv9XDGwg8y8+ZBvXpsO1qe6dPhvvtAJNBBGWPMGenqJVVVvwa+9pneBtx5vnVEJAQYAbQFIoFVIjJdVTemWHSiqj6eoaiD0bFjsHQpDBzI0KEQEoINqGOMyXbSW9FcSUSmisifIrJPRCaLSKULrNYEiFDVbap6EpgAdLnYgIPWokVw8iQHmnZgzBi45x6oWDHQQRljzNnSW3w0FpgOXA5UBGZ4886nIrDbZzrSm5fSnSKyTkQmiUjl1DYkIn1FZLWIrI6JiUlnyNnM/PlQoAAfbriBY8fg2WcDHZAxxpwrvUmhrKqOVdUE7/UpUPYC66RWWq4ppmcA1VS1HrAA+Cy1DanqKFUNV9XwsmUv9LXZ1Pz5xDdrxXsj89G+PYSFBTogY4w5V3qTwn4R6S0iId6rNxB7gXUiAd8r/0pAlO8CqhqrqvHe5MdA43TGE1z27oX165le4VH27YOnngp0QMYYk7r0JoUHcc1R9wLRQDdc1xfnswqoJSLVRSQ/cDeuCCqZ18w1SWdgUzrjCS4LFgAwbk9LLr8cWrcOcDzGGJOG9LY+2oU7aScTkSeBoedZJ0FEHsc1ZQ0BPlHVDSLyCrBaVacDA0WkM5AAHAD6ZGovsrv58zlY6gpmLSvBgAGu5ZExxmRHopqymD+dK4rsUtUqWRzPBYWHh+vq1asv9ddmXkICVKjA6Bqv8cjKvqxeDY1zZiGZMSYbE5E1qhp+oeUuZrwve+wqPZYsgf37GXf0dq68Eho1CnRAxhiTtotJCpm7xchtpkwhssAV/LixLL162RPMxpjs7bx1CiJyhNRP/gIU8ktEOUliIkyZwsSar6IbhJ49Ax2QMcac33mTgqoWu1SB5EirVsGePUws0InwcKhVK9ABGWPM+V1M8ZG5kClTiA6pxKptZbjjjkAHY4wxF2ZJwV9UYcoUZl39NAAdOwY4HmOMSQdLCv6yfj1ERDAz3+1UrgzXXBPogIwx5sIsKfjL+PGcyFOY+b9XpVMna3VkjAkOlhT8ITERxo3jh8bPcPRYHis6MsYEDUsK/rBoEezezczQ3hQuDK1aBTogY4xJH0sK/vDFF2iRoszcUpM2baBgwUAHZIwx6WNJIaudOAFff82G1gPZuSsPnToFOiBjjEk/SwpZbcYMOHyY6WVcz+K33hrgeIwxJgMsKWS1L76Ayy9n2voraNIELr880AEZY0z6WVLISnv3wqxZRHV5jJWrhM6dL7yKMcZkJ5YUstJnn0FCAjMrPAJAly4BjscYYzLIkkJWUYXRo+HGG5m2vDw1akDduoEOyhhjMsaSQlb58UeIiOCv3v347jvo3NmeYjbGBB+/JgURaS8iW0QkQkQGnWe5biKiInLBoeKyrY8/hhIlmFe0K/HxVnRkjAlOfksKIhICjAA6AHWAniJSJ5XligEDgRX+isXvDhyAyZOhd2+mzSlAqVLQokWggzLGmIzz551CEyBCVbep6klgApDa9fOrwJvACT/G4l/jxkF8PId6PMo337husvOed/giY4zJnvyZFCoCu32mI715yUSkIVBZVWeeb0Mi0ldEVovI6piYmKyP9GIkVTA3bsy7C67h8GF4+ulAB2WMMZnjz6SQWjVr8njPIpIHeBd45kIbUtVRqhququFly5bNwhCzwM8/w7p1HOz5GEOHQteuUL9+oIMyxpjM8WdSiAQq+0xXAqJ8posBYcAPIrIDuA6YHnSVzWPGQMGCvLuvF4cPw8svBzogY4zJPH8mhVVALRGpLiL5gbuB6UkfqmqcqpZR1WqqWg1YDnRW1dV+jClrHT8OX37JgY73Meyjgtx5J9SrF+igjDEm8/yWFFQ1AXgcmAtsAr5S1Q0i8oqI5IwOICZPhrg4/lfuWQ4fhpdeCnRAxhhzcfzaRkZVZwGzUsxL9dSpqi39GYtffPIJ1KjBj3tqcsUVdpdgjAl+9kRzZkVFwcKF6P19WLpMaN480AEZY8zFs6SQWdNd9cjWxncTE2MPqxljcgZLCpk1bRrUrMnSfTUB7E7BGJMjWFLIjMOH4bvv4PbbWbJUKFUKrr460EEZY8zFs6SQGXPmwKlT0KULS5dCs2aQx35JY0wOYKeyzPjmGyhblpia17Nli9UnGGNyDksKGXXyJMyaBZ06sWxFCGD1CcaYnMOSQkb9+CPExbn6hCWQPz+EB1fHHMYYkyZLChn1zTdQuDC0acPSpS4hFCwY6KCMMSZrWFLIiNOnYcoUuO029h8txOrVcMMNgQ7KGGOyjiWFjFi6FPbuhW7dGDXKNUC6775AB2WMMVnHkkJGTJoEBQtyqu2tfPABtG0Ldc4ZYNQYY4KXDRqZXomJrlfUW29l8tyi7NkDH34Y6KCMMSZr2Z1Cev30k+sEr1s3hg2DmjXh1lsDHZQxxmQtSwrpNWkSFCjAyss6sXw5DBhgTzEbY3IeO62lR2KiSwq33MLYr4pSpAj06RPooIwxJutZUkiP77+HyEjo0YO5c6F1ayhePNBBGWNM1vNrUhCR9iKyRUQiRGRQKp/3E5H1IrJWRJaISPZsy/P++1C2LH80uJPt26Fdu0AHZIwx/uG3pCAiIcAIoANQB+iZykn/S1W9RlUbAG8C7/grnkzbtg1mzIBHH2XejwUASwrGmJzLn3cKTYAIVd2mqieBCUAX3wVU9bDPZBFA/RhP5nzwAYSEQL9+zJsHVau6lkfGGJMT+fM5hYrAbp/pSKBpyoVEpD/wNJAfuNmP8WTc0aMwZgx07UpC+Yp8/z306AEigQ7MGGP8w593CqmdOs+5E1DVEap6BfA8MDjVDYn0FZHVIrI6JiYmi8M8jy++gEOHYOBAVq50A65Z0ZExJifzZ1KIBCr7TFcCos6z/ATg9tQ+UNVRqhququFly5bNwhDPQ9UVHTVoAM2aMW+eey7h5ux1L2OMMVnKn0lhFVBLRKqLSH7gbmC67wIiUstn8jZgqx/jyZg1a2DdOujbF0SYP991k12qVKADM8YY//FbUlDVBOBxYC6wCfhKVTeIyCsi0tlb7HER2SAia3H1Cvf7K54MGz0aChWCnj05dAhWrLCiI2NMzufXDvFUdRYwK8W8l3zeP+HP78+0o0dh/Hjo3h1CQ/nmUzeUQqdOgQ7MGGP8y55oTs2kSa5W+eGHAZg4EapXh2uvDXBcxhjjZ5YUUjN6NFx5JbRoQWwsLFgAd91lTVGNMTmfJYWUtmyBJUvcXYIIU6ZAQoJ7PsEYY3I6SwopjR3rnmD2xtmcOBFq1XItU40xJqezpODr9Gn4/HM3ek758uzbBwsX2lPMxpjcw5KCr/nz3ehq3mAJkye7oRSs6MgYk1tYUvA1diyULg0dO5KQAMOHQ1gY1K0b6MCMMebS8OtzCkHl4EH45hvo1w/y5+fjkbBpE0ydakVHxpjcw+4UkkyYACdPQp8+HD4ML78MN90EXbpceFVjjMkpLCmA6/xu9GioVw8aNOD11yEmBt5+2+4SjDG5ixUfASxdCj//DB9+yJ4o4d134d57oXHjQAdmjDGXlt0pAAwdCiVLwr33MmkSxMfD4FRHdjDGmJzNksKOHa42+dFHoXBhpk2DOnVcLxfGGJPbWFIYPtxVHPTvz8GDsGiRVS4bY3Kv3J0UjhxxFczdu0OlSsya5R5qtqRgjMmtcndSmDwZ4uJg4EAApk2Dyy6zLrKNMblX7k4Kc+e6LHDddcTHw5w5biCdPLn7VzHG5GK59/SXmOj6OmrbFkT44QdXmmRFR8aY3MyvSUFE2ovIFhGJEJFBqXz+tIhsFJF1IvKdiFT1Zzxn+eUXiI1NHnh5+nQoXBhuvvmSRWCMMdmO35KCiIQAI4AOQB2gp4jUSbHYL0C4qtYDJgFv+iuec8yf7/5t04a4OPjyS+jYEQoVumQRGGNMtuPPO4UmQISqblPVk8AE4KzCGVVdqKrHvMnlQCU/xnO2efOgfn247DKGDYNDh2DQOfcyxhiTu/gzKVQEdvtMR3rz0vIQMDu1D0Skr4isFpHVMTExFx/Z0aNuyM22bTl0CN59F26/HRo2vPhNG2NMMPNnUkitKzlNdUGR3kA48FZqn6vqKFUNV9XwsmXLXnxkP/4Ip05Bu3bJdwkvvXTxmzXGmGDnzw7xIoHKPtOVgKiUC4lIG+BF4CZVjfdjPGfMnw8FC3IorAXvdre7BGOMSeLPO4VVQC0RqS4i+YG7gem+C4hIQ+AjoLOq/unHWM42bx6nb2jJ/f0KceSIGzvBGGOMH+8UVDVBRB4H5gIhwCequkFEXgFWq+p0XHFRUeBrcQMX7FLVzv6KCYC9e2HjRp4p+RnT58P770ODBn79RmOMCRp+HU9BVWcBs1LMe8nnfRt/fn+qfvyR93mcYUvDeeopePzxSx6BMcZkW7nuieYTC5bwAq9za4dE3kq1WtsYY3KvXDfy2sLZJzhKUQYMhJCQQEdjjDHZS+66U4iOZuaeBhTOd4qWLQMdjDHGZD+5KinoDz8yk460bfYXBQsGOhpjjMl+clVS+G3KFnZRlU69SgQ6FGOMyZZyVVKYsbAYALd2zFW7bYwx6ZZ7zo5RUcyMvY7wynupUCHQwRhjTPaUa5JCzPSfWM51dLw1MdChGGNMtpVrksLstZeh5KHTQ+UDHYoxxmRbueY5hdBbm3P7PmgYbg8nGGNMWnJNUujc2b2MMcakLdcUHxljjLkwSwrGGGOSWVIwxhiTzJKCMcaYZJYUjDHGJLOkYIwxJpklBWOMMcksKRhjjEkmqhroGDJERGKAnRlcrQyw3w/hBILtS/Zk+5J95aT9uZh9qaqqZS+0UNAlhcwQkdWqGh7oOLKC7Uv2ZPuSfeWk/bkU+2LFR8YYY5JZUjDGGJMstySFUYEOIAvZvmRPti/ZV07aH7/vS66oUzDGGJM+ueVOwRhjTDpYUjDGGJMsRycFEWkvIltEJEJEBgU6nowQkcoislBENonIBhF5wptfSkTmi8hW79+SgY41vUQkRER+EZGZ3nR1EVnh7ctEEckf6BjTS0RCRWSSiGz2jtH1wXpsROQp72/sNxEZLyIFg+XYiMgnIvKniPzmMy/V4yDOe975YJ2INApc5OdKY1/e8v7G1onIVBEJ9fnsBW9ftojILVkVR45NCiISAowAOgB1gJ4iUiewUWVIAvCMqtYGrgP6e/EPAr5T1VrAd950sHgC2OQz/R/gXW9fDgIPBSSqzBkGzFHVq4H6uP0KumMjIhWBgUC4qoYBIcDdBM+x+RRon2JeWsehA1DLe/UFRl6iGNPrU87dl/lAmKrWA34HXgDwzgV3A3W9dT7wznkXLccmBaAJEKGq21T1JDAB6BLgmNJNVaNV9Wfv/RHcSacibh8+8xb7DLg9MBFmjIhUAm4DRnvTAtwMTPIWCaZ9KQ7cCIwBUNWTqnqIID02uGF5C4lIXqAwEE2QHBtVXQQcSDE7rePQBfifOsuBUBGpcGkivbDU9kVV56lqgje5HKjkve8CTFDVeFXdDkTgznkXLScnhYrAbp/pSG9e0BGRakBDYAVQXlWjwSUOoFzgIsuQocDfgURvujRwyOcPPpiOTw0gBhjrFYeNFpEiBOGxUdU9wH+BXbhkEAesIXiPDaR9HIL9nPAgMNt777d9yclJQVKZF3Ttb0WkKDAZeFJVDwc6nswQkY7An6q6xnd2KosGy/HJCzQCRqpqQ+AoQVBUlBqvvL0LUB24HCiCK2ZJKViOzfkE7d+ciLyIK1IelzQrlcWyZF9yclKIBCr7TFcCogIUS6aISD5cQhinqlO82fuSbnm9f/8MVHwZ0BzoLCI7cMV4N+PuHEK9IgsIruMTCUSq6gpvehIuSQTjsWkDbFfVGFU9BUwBmhG8xwbSPg5BeU4QkfuBjkAvPfNgmd/2JScnhVVALa8VRX5cpcz0AMeUbl6Z+xhgk6q+4/PRdOB+7/39wLRLHVtGqeoLqlpJVavhjsP3qtoLWAh08xYLin0BUNW9wG4Rucqb1RrYSBAeG1yx0XUiUtj7m0val6A8Np60jsN04D6vFdJ1QFxSMVN2JSLtgeeBzqp6zOej6cDdIlJARKrjKs9XZsmXqmqOfQG34mrs/wBeDHQ8GYy9Be52cB2w1nvdiiuL/w7Y6v1bKtCxZnC/WgIzvfc1vD/kCOBroECg48vAfjQAVnvH5xugZLAeG+CfwGbgN+BzoECwHBtgPK4u5BTu6vmhtI4DrshlhHc+WI9rcRXwfbjAvkTg6g6SzgEf+iz/orcvW4AOWRWHdXNhjDEmWU4uPjLGGJNBlhSMMcYks6RgjDEmmSUFY4wxySwpGGOMSWZJwRiPiJwWkbU+ryx7SllEqvn2fmlMdpX3wosYk2scV9UGgQ7CmECyOwVjLkBEdojIf0Rkpfeq6c2vKiLfeX3dfyciVbz55b2+73/1Xs28TYWIyMfe2AXzRKSQt/xAEdnobWdCgHbTGMCSgjG+CqUoPurh89lhVW0CDMf124T3/n/q+rofB7znzX8P+FFV6+P6RNrgza8FjFDVusAh4E5v/iCgobedfv7aOWPSw55oNsYjIn+patFU5u8AblbVbV4nhXtVtbSI7AcqqOopb360qpYRkRigkqrG+2yjGjBf3cAviMjzQD5V/ZeIzAH+wnWX8Y2q/uXnXTUmTXanYEz6aBrv01omNfE+709zpk7vNlyfPI2BNT69kxpzyVlSMCZ9evj8+5P3fhmu11eAXsAS7/13wGOQPC518bQ2KiJ5gMqquhA3CFEocM7dijGXil2RGHNGIRFZ6zM9R1WTmqUWEJEVuAupnt68gcAnIvIcbiS2B7z5TwCjROQh3B3BY7jeL1MTAnwhIiVwvXi+q25oT2MCwuoUjLkAr04hXFX3BzoWY/zNio+MMcYkszsFY4wxyexOwRhjTDJLCsYYY5JZUjDGGJPMkoIxxphklhSMMcYk+38bubV0Nc4+yQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9269 - acc: 0.1735 - val_loss: 1.9049 - val_acc: 0.2110\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8956 - acc: 0.2239 - val_loss: 1.8786 - val_acc: 0.2420\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8697 - acc: 0.2552 - val_loss: 1.8503 - val_acc: 0.2690\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8411 - acc: 0.2837 - val_loss: 1.8184 - val_acc: 0.2930\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8081 - acc: 0.3112 - val_loss: 1.7820 - val_acc: 0.3300\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7696 - acc: 0.3436 - val_loss: 1.7402 - val_acc: 0.3630\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7254 - acc: 0.3719 - val_loss: 1.6942 - val_acc: 0.3850\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6764 - acc: 0.4005 - val_loss: 1.6444 - val_acc: 0.4220\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6236 - acc: 0.4239 - val_loss: 1.5916 - val_acc: 0.4440\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5683 - acc: 0.4529 - val_loss: 1.5375 - val_acc: 0.4640\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5127 - acc: 0.4808 - val_loss: 1.4839 - val_acc: 0.4920\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4575 - acc: 0.5065 - val_loss: 1.4315 - val_acc: 0.5260\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4034 - acc: 0.5376 - val_loss: 1.3821 - val_acc: 0.5480\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3515 - acc: 0.5636 - val_loss: 1.3348 - val_acc: 0.5740\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3019 - acc: 0.5867 - val_loss: 1.2893 - val_acc: 0.5880\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2549 - acc: 0.6056 - val_loss: 1.2469 - val_acc: 0.6050\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2092 - acc: 0.6281 - val_loss: 1.2061 - val_acc: 0.6140\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1665 - acc: 0.6452 - val_loss: 1.1677 - val_acc: 0.6250\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1260 - acc: 0.6591 - val_loss: 1.1320 - val_acc: 0.6320\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0876 - acc: 0.6712 - val_loss: 1.0975 - val_acc: 0.6430\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0514 - acc: 0.6813 - val_loss: 1.0677 - val_acc: 0.6510\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0176 - acc: 0.6920 - val_loss: 1.0371 - val_acc: 0.6600\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9856 - acc: 0.6997 - val_loss: 1.0105 - val_acc: 0.6680\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9559 - acc: 0.7052 - val_loss: 0.9842 - val_acc: 0.6740\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9276 - acc: 0.7144 - val_loss: 0.9604 - val_acc: 0.6840\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9016 - acc: 0.7199 - val_loss: 0.9392 - val_acc: 0.6790\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8772 - acc: 0.7257 - val_loss: 0.9181 - val_acc: 0.6810\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8546 - acc: 0.7304 - val_loss: 0.8994 - val_acc: 0.6800\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.7336 - val_loss: 0.8829 - val_acc: 0.6940\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8141 - acc: 0.7396 - val_loss: 0.8653 - val_acc: 0.6930\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7956 - acc: 0.7419 - val_loss: 0.8524 - val_acc: 0.7080\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7785 - acc: 0.7477 - val_loss: 0.8369 - val_acc: 0.7060\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7625 - acc: 0.7483 - val_loss: 0.8240 - val_acc: 0.7100\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7476 - acc: 0.7529 - val_loss: 0.8120 - val_acc: 0.7120\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7335 - acc: 0.7552 - val_loss: 0.8007 - val_acc: 0.7110\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7202 - acc: 0.7599 - val_loss: 0.7904 - val_acc: 0.7100\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7076 - acc: 0.7648 - val_loss: 0.7809 - val_acc: 0.7130\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6960 - acc: 0.7668 - val_loss: 0.7718 - val_acc: 0.7180\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6846 - acc: 0.7699 - val_loss: 0.7639 - val_acc: 0.7170\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6744 - acc: 0.7713 - val_loss: 0.7551 - val_acc: 0.7190\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6639 - acc: 0.7761 - val_loss: 0.7501 - val_acc: 0.7210\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6548 - acc: 0.7783 - val_loss: 0.7417 - val_acc: 0.7250\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6453 - acc: 0.7795 - val_loss: 0.7367 - val_acc: 0.7270\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6364 - acc: 0.7824 - val_loss: 0.7295 - val_acc: 0.7250\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6282 - acc: 0.7849 - val_loss: 0.7241 - val_acc: 0.7240\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6202 - acc: 0.7868 - val_loss: 0.7177 - val_acc: 0.7310\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6126 - acc: 0.7887 - val_loss: 0.7124 - val_acc: 0.7290\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6052 - acc: 0.7907 - val_loss: 0.7082 - val_acc: 0.7240\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5983 - acc: 0.7949 - val_loss: 0.7045 - val_acc: 0.7250\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5911 - acc: 0.7948 - val_loss: 0.6993 - val_acc: 0.7310\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5845 - acc: 0.8011 - val_loss: 0.6966 - val_acc: 0.7370\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5777 - acc: 0.8020 - val_loss: 0.6916 - val_acc: 0.7340\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5717 - acc: 0.8015 - val_loss: 0.6878 - val_acc: 0.7360\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5656 - acc: 0.8033 - val_loss: 0.6828 - val_acc: 0.7390\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5599 - acc: 0.8056 - val_loss: 0.6801 - val_acc: 0.7390\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5543 - acc: 0.8056 - val_loss: 0.6778 - val_acc: 0.7410\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5489 - acc: 0.8081 - val_loss: 0.6767 - val_acc: 0.7410\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5433 - acc: 0.8117 - val_loss: 0.6723 - val_acc: 0.7430\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5376 - acc: 0.8125 - val_loss: 0.6701 - val_acc: 0.7360\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5328 - acc: 0.8125 - val_loss: 0.6668 - val_acc: 0.7370\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5284558980464935, 0.8126666666666666]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6203860843976339, 0.7733333330154419]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.6313 - acc: 0.1344 - val_loss: 2.6019 - val_acc: 0.1590\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5976 - acc: 0.1447 - val_loss: 2.5841 - val_acc: 0.1680\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5803 - acc: 0.1612 - val_loss: 2.5699 - val_acc: 0.1790\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5651 - acc: 0.1795 - val_loss: 2.5561 - val_acc: 0.1920\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5501 - acc: 0.1968 - val_loss: 2.5422 - val_acc: 0.2140\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.5345 - acc: 0.2205 - val_loss: 2.5272 - val_acc: 0.2310\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5176 - acc: 0.2387 - val_loss: 2.5105 - val_acc: 0.2590\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.4980 - acc: 0.2623 - val_loss: 2.4888 - val_acc: 0.2790\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4740 - acc: 0.2844 - val_loss: 2.4631 - val_acc: 0.3050\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.4465 - acc: 0.3052 - val_loss: 2.4333 - val_acc: 0.3280\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.4147 - acc: 0.3387 - val_loss: 2.3993 - val_acc: 0.3750\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3790 - acc: 0.3801 - val_loss: 2.3622 - val_acc: 0.4150\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3393 - acc: 0.4172 - val_loss: 2.3223 - val_acc: 0.4520\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2960 - acc: 0.4596 - val_loss: 2.2775 - val_acc: 0.4850\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2494 - acc: 0.4956 - val_loss: 2.2311 - val_acc: 0.5090\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1997 - acc: 0.5280 - val_loss: 2.1819 - val_acc: 0.5310\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1478 - acc: 0.5509 - val_loss: 2.1305 - val_acc: 0.5460\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0941 - acc: 0.5740 - val_loss: 2.0777 - val_acc: 0.5640\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0401 - acc: 0.5879 - val_loss: 2.0265 - val_acc: 0.5860\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9866 - acc: 0.6052 - val_loss: 1.9767 - val_acc: 0.5970\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9343 - acc: 0.6160 - val_loss: 1.9272 - val_acc: 0.6070\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8844 - acc: 0.6248 - val_loss: 1.8815 - val_acc: 0.6160\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8362 - acc: 0.6375 - val_loss: 1.8365 - val_acc: 0.6310\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7908 - acc: 0.6453 - val_loss: 1.7975 - val_acc: 0.6320\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7478 - acc: 0.6537 - val_loss: 1.7570 - val_acc: 0.6370\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7078 - acc: 0.6615 - val_loss: 1.7224 - val_acc: 0.6400\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6704 - acc: 0.6692 - val_loss: 1.6898 - val_acc: 0.6480\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6352 - acc: 0.6788 - val_loss: 1.6578 - val_acc: 0.6490\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6029 - acc: 0.6835 - val_loss: 1.6290 - val_acc: 0.6560\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5729 - acc: 0.6895 - val_loss: 1.6042 - val_acc: 0.6570\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5445 - acc: 0.6977 - val_loss: 1.5793 - val_acc: 0.6620\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5181 - acc: 0.7075 - val_loss: 1.5559 - val_acc: 0.6640\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4938 - acc: 0.7068 - val_loss: 1.5350 - val_acc: 0.6700\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4709 - acc: 0.7117 - val_loss: 1.5179 - val_acc: 0.6720\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4498 - acc: 0.7183 - val_loss: 1.4973 - val_acc: 0.6800\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4300 - acc: 0.7237 - val_loss: 1.4808 - val_acc: 0.6820\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4109 - acc: 0.7271 - val_loss: 1.4655 - val_acc: 0.6810\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3936 - acc: 0.7341 - val_loss: 1.4506 - val_acc: 0.6890\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3765 - acc: 0.7377 - val_loss: 1.4373 - val_acc: 0.6900\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3611 - acc: 0.7377 - val_loss: 1.4228 - val_acc: 0.6980\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3455 - acc: 0.7429 - val_loss: 1.4108 - val_acc: 0.7020\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3315 - acc: 0.7465 - val_loss: 1.3996 - val_acc: 0.7070\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3180 - acc: 0.7501 - val_loss: 1.3892 - val_acc: 0.7040\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3048 - acc: 0.7537 - val_loss: 1.3780 - val_acc: 0.7050\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2924 - acc: 0.7560 - val_loss: 1.3710 - val_acc: 0.7120\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2808 - acc: 0.7596 - val_loss: 1.3597 - val_acc: 0.7150\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2690 - acc: 0.7621 - val_loss: 1.3530 - val_acc: 0.7180\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2585 - acc: 0.7661 - val_loss: 1.3418 - val_acc: 0.7140\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2481 - acc: 0.7671 - val_loss: 1.3352 - val_acc: 0.7120\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2384 - acc: 0.7708 - val_loss: 1.3268 - val_acc: 0.7150\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2280 - acc: 0.7739 - val_loss: 1.3186 - val_acc: 0.7160\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2184 - acc: 0.7759 - val_loss: 1.3110 - val_acc: 0.7180\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2096 - acc: 0.7788 - val_loss: 1.3041 - val_acc: 0.7250\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2006 - acc: 0.7793 - val_loss: 1.2973 - val_acc: 0.7230\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1919 - acc: 0.7807 - val_loss: 1.2917 - val_acc: 0.7290\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1837 - acc: 0.7833 - val_loss: 1.2861 - val_acc: 0.7300\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1758 - acc: 0.7865 - val_loss: 1.2793 - val_acc: 0.7300\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1672 - acc: 0.7879 - val_loss: 1.2743 - val_acc: 0.7290\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1595 - acc: 0.7899 - val_loss: 1.2684 - val_acc: 0.7330\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1523 - acc: 0.7924 - val_loss: 1.2627 - val_acc: 0.7360\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1449 - acc: 0.7952 - val_loss: 1.2572 - val_acc: 0.7330\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1374 - acc: 0.7999 - val_loss: 1.2522 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1305 - acc: 0.7996 - val_loss: 1.2477 - val_acc: 0.7350\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1237 - acc: 0.7995 - val_loss: 1.2426 - val_acc: 0.7440\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1166 - acc: 0.8004 - val_loss: 1.2370 - val_acc: 0.7380\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1101 - acc: 0.8039 - val_loss: 1.2332 - val_acc: 0.7400\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1032 - acc: 0.8055 - val_loss: 1.2295 - val_acc: 0.7450\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0970 - acc: 0.8052 - val_loss: 1.2239 - val_acc: 0.7470\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0906 - acc: 0.8075 - val_loss: 1.2204 - val_acc: 0.7370\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0847 - acc: 0.8085 - val_loss: 1.2195 - val_acc: 0.7400\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0783 - acc: 0.8104 - val_loss: 1.2127 - val_acc: 0.7410\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0721 - acc: 0.8125 - val_loss: 1.2080 - val_acc: 0.7370\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0661 - acc: 0.8133 - val_loss: 1.2063 - val_acc: 0.7490\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0606 - acc: 0.8131 - val_loss: 1.2008 - val_acc: 0.7510\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0547 - acc: 0.8152 - val_loss: 1.1955 - val_acc: 0.7410\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0492 - acc: 0.8171 - val_loss: 1.1909 - val_acc: 0.7470\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0430 - acc: 0.8175 - val_loss: 1.1886 - val_acc: 0.7530\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0382 - acc: 0.8197 - val_loss: 1.1845 - val_acc: 0.7530\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0324 - acc: 0.8216 - val_loss: 1.1818 - val_acc: 0.7520\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0274 - acc: 0.8227 - val_loss: 1.1773 - val_acc: 0.7540\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0215 - acc: 0.8244 - val_loss: 1.1739 - val_acc: 0.7520\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0162 - acc: 0.8260 - val_loss: 1.1702 - val_acc: 0.7460\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0116 - acc: 0.8261 - val_loss: 1.1669 - val_acc: 0.7560\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0065 - acc: 0.8269 - val_loss: 1.1640 - val_acc: 0.7550\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0013 - acc: 0.8297 - val_loss: 1.1632 - val_acc: 0.7600\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9966 - acc: 0.8316 - val_loss: 1.1589 - val_acc: 0.7630\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9916 - acc: 0.8313 - val_loss: 1.1546 - val_acc: 0.7570\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9867 - acc: 0.8344 - val_loss: 1.1536 - val_acc: 0.7610\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9822 - acc: 0.8341 - val_loss: 1.1484 - val_acc: 0.7620\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9772 - acc: 0.8367 - val_loss: 1.1458 - val_acc: 0.7600\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9725 - acc: 0.8375 - val_loss: 1.1440 - val_acc: 0.7600\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9678 - acc: 0.8384 - val_loss: 1.1408 - val_acc: 0.7590\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9633 - acc: 0.8389 - val_loss: 1.1369 - val_acc: 0.7610\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9587 - acc: 0.8404 - val_loss: 1.1352 - val_acc: 0.7610\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9544 - acc: 0.8423 - val_loss: 1.1318 - val_acc: 0.7650\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9501 - acc: 0.8432 - val_loss: 1.1297 - val_acc: 0.7620\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9454 - acc: 0.8449 - val_loss: 1.1267 - val_acc: 0.7640\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9413 - acc: 0.8476 - val_loss: 1.1243 - val_acc: 0.7660\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9370 - acc: 0.8469 - val_loss: 1.1215 - val_acc: 0.7680\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9326 - acc: 0.8496 - val_loss: 1.1190 - val_acc: 0.7640\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9289 - acc: 0.8477 - val_loss: 1.1202 - val_acc: 0.7550\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9244 - acc: 0.8491 - val_loss: 1.1155 - val_acc: 0.7710\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9207 - acc: 0.8525 - val_loss: 1.1143 - val_acc: 0.7690\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9164 - acc: 0.8520 - val_loss: 1.1098 - val_acc: 0.7730\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9121 - acc: 0.8533 - val_loss: 1.1133 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9083 - acc: 0.8551 - val_loss: 1.1048 - val_acc: 0.7750\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9044 - acc: 0.8539 - val_loss: 1.1026 - val_acc: 0.7690\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9007 - acc: 0.8571 - val_loss: 1.1003 - val_acc: 0.7690\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8966 - acc: 0.8589 - val_loss: 1.0988 - val_acc: 0.7690\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8930 - acc: 0.8576 - val_loss: 1.0964 - val_acc: 0.7730\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8890 - acc: 0.8592 - val_loss: 1.0941 - val_acc: 0.7710\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8848 - acc: 0.8613 - val_loss: 1.0923 - val_acc: 0.7730\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8813 - acc: 0.8620 - val_loss: 1.0902 - val_acc: 0.7710\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8779 - acc: 0.8619 - val_loss: 1.0890 - val_acc: 0.7640\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8743 - acc: 0.8637 - val_loss: 1.0858 - val_acc: 0.7730\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8708 - acc: 0.8649 - val_loss: 1.0851 - val_acc: 0.7710\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8671 - acc: 0.8665 - val_loss: 1.0829 - val_acc: 0.7710\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8633 - acc: 0.8669 - val_loss: 1.0800 - val_acc: 0.7730\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8601 - acc: 0.8655 - val_loss: 1.0794 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8565 - acc: 0.8676 - val_loss: 1.0793 - val_acc: 0.7710\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 15.9780 - acc: 0.1759 - val_loss: 15.5695 - val_acc: 0.1980\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 15.2192 - acc: 0.2005 - val_loss: 14.8252 - val_acc: 0.2170\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 14.4851 - acc: 0.2257 - val_loss: 14.1023 - val_acc: 0.2400\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 13.7713 - acc: 0.2472 - val_loss: 13.4007 - val_acc: 0.2730\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 13.0783 - acc: 0.2712 - val_loss: 12.7190 - val_acc: 0.2860\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 12.4045 - acc: 0.2933 - val_loss: 12.0551 - val_acc: 0.3060\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 11.7485 - acc: 0.3189 - val_loss: 11.4103 - val_acc: 0.3260\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.1107 - acc: 0.3483 - val_loss: 10.7830 - val_acc: 0.3460\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 10.4913 - acc: 0.3760 - val_loss: 10.1741 - val_acc: 0.3710\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 9.8904 - acc: 0.4004 - val_loss: 9.5842 - val_acc: 0.4190\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 9.3094 - acc: 0.4235 - val_loss: 9.0147 - val_acc: 0.4440\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.7490 - acc: 0.4489 - val_loss: 8.4673 - val_acc: 0.4720\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.2108 - acc: 0.4752 - val_loss: 7.9397 - val_acc: 0.4850\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.6949 - acc: 0.4931 - val_loss: 7.4396 - val_acc: 0.4980\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.2028 - acc: 0.5193 - val_loss: 6.9577 - val_acc: 0.5180\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 6.7329 - acc: 0.5412 - val_loss: 6.5003 - val_acc: 0.5350\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.2864 - acc: 0.5668 - val_loss: 6.0676 - val_acc: 0.5510\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.8639 - acc: 0.5825 - val_loss: 5.6599 - val_acc: 0.5700\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.4657 - acc: 0.5995 - val_loss: 5.2750 - val_acc: 0.5800\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 5.0919 - acc: 0.6153 - val_loss: 4.9138 - val_acc: 0.5900\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.7421 - acc: 0.6255 - val_loss: 4.5759 - val_acc: 0.6050\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.4152 - acc: 0.6348 - val_loss: 4.2619 - val_acc: 0.6220\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.1114 - acc: 0.6473 - val_loss: 3.9711 - val_acc: 0.6180\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 3.8308 - acc: 0.6521 - val_loss: 3.7027 - val_acc: 0.6340\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.5729 - acc: 0.6600 - val_loss: 3.4596 - val_acc: 0.6370\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.3377 - acc: 0.6617 - val_loss: 3.2332 - val_acc: 0.6530\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1234 - acc: 0.6683 - val_loss: 3.0300 - val_acc: 0.6540\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.9308 - acc: 0.6697 - val_loss: 2.8481 - val_acc: 0.6710\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.7597 - acc: 0.6745 - val_loss: 2.6902 - val_acc: 0.6770\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.6095 - acc: 0.6755 - val_loss: 2.5475 - val_acc: 0.6700\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4796 - acc: 0.6768 - val_loss: 2.4285 - val_acc: 0.6720\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.3691 - acc: 0.6796 - val_loss: 2.3302 - val_acc: 0.6710\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.2779 - acc: 0.6815 - val_loss: 2.2450 - val_acc: 0.6890\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2047 - acc: 0.6836 - val_loss: 2.1807 - val_acc: 0.6870\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1470 - acc: 0.6852 - val_loss: 2.1295 - val_acc: 0.6870\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1036 - acc: 0.6872 - val_loss: 2.0925 - val_acc: 0.6820\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0714 - acc: 0.6871 - val_loss: 2.0637 - val_acc: 0.6820\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0451 - acc: 0.6853 - val_loss: 2.0400 - val_acc: 0.6900\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0223 - acc: 0.6868 - val_loss: 2.0165 - val_acc: 0.6860\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0019 - acc: 0.6872 - val_loss: 1.9980 - val_acc: 0.6960\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9828 - acc: 0.6896 - val_loss: 1.9803 - val_acc: 0.6950\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9653 - acc: 0.6895 - val_loss: 1.9632 - val_acc: 0.6960\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9482 - acc: 0.6912 - val_loss: 1.9460 - val_acc: 0.6950\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9322 - acc: 0.6892 - val_loss: 1.9280 - val_acc: 0.7000\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9163 - acc: 0.6927 - val_loss: 1.9133 - val_acc: 0.6930\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9016 - acc: 0.6937 - val_loss: 1.8987 - val_acc: 0.6990\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8871 - acc: 0.6931 - val_loss: 1.8861 - val_acc: 0.6970\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8731 - acc: 0.6939 - val_loss: 1.8714 - val_acc: 0.6960\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8593 - acc: 0.6928 - val_loss: 1.8612 - val_acc: 0.6930\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8467 - acc: 0.6948 - val_loss: 1.8465 - val_acc: 0.6940\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8337 - acc: 0.6947 - val_loss: 1.8322 - val_acc: 0.7000\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8213 - acc: 0.6935 - val_loss: 1.8197 - val_acc: 0.6960\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8090 - acc: 0.6952 - val_loss: 1.8074 - val_acc: 0.6960\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7973 - acc: 0.6960 - val_loss: 1.7983 - val_acc: 0.6990\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7862 - acc: 0.6980 - val_loss: 1.7852 - val_acc: 0.6960\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7748 - acc: 0.6963 - val_loss: 1.7774 - val_acc: 0.7080\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7640 - acc: 0.6955 - val_loss: 1.7632 - val_acc: 0.7000\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7530 - acc: 0.6961 - val_loss: 1.7523 - val_acc: 0.6980\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7426 - acc: 0.6972 - val_loss: 1.7432 - val_acc: 0.7020\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7326 - acc: 0.6983 - val_loss: 1.7331 - val_acc: 0.7030\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7230 - acc: 0.6972 - val_loss: 1.7240 - val_acc: 0.7040\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7132 - acc: 0.6983 - val_loss: 1.7136 - val_acc: 0.7020\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7034 - acc: 0.6992 - val_loss: 1.7069 - val_acc: 0.7040\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6944 - acc: 0.7000 - val_loss: 1.6962 - val_acc: 0.7000\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6853 - acc: 0.7007 - val_loss: 1.6880 - val_acc: 0.6980\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6765 - acc: 0.7016 - val_loss: 1.6780 - val_acc: 0.7010\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6671 - acc: 0.7012 - val_loss: 1.6708 - val_acc: 0.7050\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6586 - acc: 0.7025 - val_loss: 1.6621 - val_acc: 0.7060\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6504 - acc: 0.7005 - val_loss: 1.6519 - val_acc: 0.7070\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6422 - acc: 0.7031 - val_loss: 1.6440 - val_acc: 0.7050\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6337 - acc: 0.7020 - val_loss: 1.6395 - val_acc: 0.7030\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6260 - acc: 0.7024 - val_loss: 1.6284 - val_acc: 0.7070\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6178 - acc: 0.7040 - val_loss: 1.6211 - val_acc: 0.7070\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6102 - acc: 0.7031 - val_loss: 1.6122 - val_acc: 0.7050\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6022 - acc: 0.7051 - val_loss: 1.6049 - val_acc: 0.7070\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5946 - acc: 0.7043 - val_loss: 1.5968 - val_acc: 0.7090\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5874 - acc: 0.7047 - val_loss: 1.5916 - val_acc: 0.7070\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5799 - acc: 0.7033 - val_loss: 1.5841 - val_acc: 0.7080\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5731 - acc: 0.7055 - val_loss: 1.5775 - val_acc: 0.7060\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5654 - acc: 0.7060 - val_loss: 1.5733 - val_acc: 0.7100\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5588 - acc: 0.7073 - val_loss: 1.5625 - val_acc: 0.7110\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5519 - acc: 0.7057 - val_loss: 1.5573 - val_acc: 0.7070\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.5449 - acc: 0.7061 - val_loss: 1.5478 - val_acc: 0.7070\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5378 - acc: 0.7087 - val_loss: 1.5422 - val_acc: 0.7100\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5309 - acc: 0.7084 - val_loss: 1.5349 - val_acc: 0.7100\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5247 - acc: 0.7064 - val_loss: 1.5300 - val_acc: 0.7100\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5182 - acc: 0.7083 - val_loss: 1.5249 - val_acc: 0.7120\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5115 - acc: 0.7084 - val_loss: 1.5197 - val_acc: 0.7090\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5061 - acc: 0.7091 - val_loss: 1.5111 - val_acc: 0.7120\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4992 - acc: 0.7097 - val_loss: 1.5089 - val_acc: 0.7040\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4934 - acc: 0.7075 - val_loss: 1.4968 - val_acc: 0.7110\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4862 - acc: 0.7093 - val_loss: 1.4915 - val_acc: 0.7140\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4804 - acc: 0.7083 - val_loss: 1.4845 - val_acc: 0.7130\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4746 - acc: 0.7084 - val_loss: 1.4818 - val_acc: 0.7130\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4684 - acc: 0.7104 - val_loss: 1.4776 - val_acc: 0.7150\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4626 - acc: 0.7107 - val_loss: 1.4695 - val_acc: 0.7090\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4572 - acc: 0.7088 - val_loss: 1.4639 - val_acc: 0.7130\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4514 - acc: 0.7093 - val_loss: 1.4579 - val_acc: 0.7150\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4455 - acc: 0.7115 - val_loss: 1.4503 - val_acc: 0.7110\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4397 - acc: 0.7113 - val_loss: 1.4440 - val_acc: 0.7110\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4340 - acc: 0.7113 - val_loss: 1.4410 - val_acc: 0.7150\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4288 - acc: 0.7120 - val_loss: 1.4347 - val_acc: 0.7120\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4231 - acc: 0.7125 - val_loss: 1.4322 - val_acc: 0.7100\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4179 - acc: 0.7113 - val_loss: 1.4258 - val_acc: 0.7230\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4121 - acc: 0.7140 - val_loss: 1.4213 - val_acc: 0.7170\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4072 - acc: 0.7119 - val_loss: 1.4139 - val_acc: 0.7140\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4014 - acc: 0.7123 - val_loss: 1.4110 - val_acc: 0.7120\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3965 - acc: 0.7131 - val_loss: 1.4098 - val_acc: 0.7140\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3920 - acc: 0.7135 - val_loss: 1.4019 - val_acc: 0.7130\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3865 - acc: 0.7121 - val_loss: 1.3965 - val_acc: 0.7160\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3812 - acc: 0.7153 - val_loss: 1.3966 - val_acc: 0.7180\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3764 - acc: 0.7136 - val_loss: 1.3856 - val_acc: 0.7160\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3714 - acc: 0.7151 - val_loss: 1.3809 - val_acc: 0.7140\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3668 - acc: 0.7145 - val_loss: 1.3726 - val_acc: 0.7190\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3612 - acc: 0.7147 - val_loss: 1.3717 - val_acc: 0.7230\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3563 - acc: 0.7159 - val_loss: 1.3662 - val_acc: 0.7170\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3515 - acc: 0.7169 - val_loss: 1.3608 - val_acc: 0.7190\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3473 - acc: 0.7169 - val_loss: 1.3557 - val_acc: 0.7180\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3425 - acc: 0.7187 - val_loss: 1.3495 - val_acc: 0.7180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3375 - acc: 0.7181 - val_loss: 1.3463 - val_acc: 0.7190\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 15.9835 - acc: 0.1357 - val_loss: 15.5744 - val_acc: 0.1520\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 15.2221 - acc: 0.1541 - val_loss: 14.8275 - val_acc: 0.1640\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 14.4867 - acc: 0.1740 - val_loss: 14.1047 - val_acc: 0.1850\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 13.7744 - acc: 0.1964 - val_loss: 13.4035 - val_acc: 0.2110\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 13.0833 - acc: 0.2220 - val_loss: 12.7235 - val_acc: 0.2390\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 12.4126 - acc: 0.2541 - val_loss: 12.0630 - val_acc: 0.2710\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 11.7617 - acc: 0.2845 - val_loss: 11.4233 - val_acc: 0.2940\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.1305 - acc: 0.3184 - val_loss: 10.8020 - val_acc: 0.3250\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 10.5182 - acc: 0.3477 - val_loss: 10.1998 - val_acc: 0.3640\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 9.9251 - acc: 0.3804 - val_loss: 9.6181 - val_acc: 0.3850\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 9.3530 - acc: 0.4100 - val_loss: 9.0585 - val_acc: 0.4130\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.8024 - acc: 0.4365 - val_loss: 8.5198 - val_acc: 0.4330\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 8.2734 - acc: 0.4572 - val_loss: 8.0008 - val_acc: 0.4560\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 7.7657 - acc: 0.4789 - val_loss: 7.5049 - val_acc: 0.4830\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 7.2801 - acc: 0.4985 - val_loss: 7.0311 - val_acc: 0.5150\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.8165 - acc: 0.5221 - val_loss: 6.5793 - val_acc: 0.5180\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.3750 - acc: 0.5335 - val_loss: 6.1520 - val_acc: 0.5300\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.9561 - acc: 0.5497 - val_loss: 5.7422 - val_acc: 0.5510\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.5591 - acc: 0.5635 - val_loss: 5.3571 - val_acc: 0.5680\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.1844 - acc: 0.5748 - val_loss: 4.9950 - val_acc: 0.5840\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 4.8325 - acc: 0.5849 - val_loss: 4.6547 - val_acc: 0.5870\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 4.5028 - acc: 0.5936 - val_loss: 4.3390 - val_acc: 0.5860\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.1957 - acc: 0.5975 - val_loss: 4.0425 - val_acc: 0.6040\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.9109 - acc: 0.6079 - val_loss: 3.7691 - val_acc: 0.6090\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.6483 - acc: 0.6149 - val_loss: 3.5231 - val_acc: 0.6110\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.4084 - acc: 0.6177 - val_loss: 3.2951 - val_acc: 0.6140\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.1909 - acc: 0.6245 - val_loss: 3.0865 - val_acc: 0.6400\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.9945 - acc: 0.6335 - val_loss: 2.9025 - val_acc: 0.6300\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.8200 - acc: 0.6365 - val_loss: 2.7367 - val_acc: 0.6320\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.6661 - acc: 0.6391 - val_loss: 2.5963 - val_acc: 0.6370\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5330 - acc: 0.6425 - val_loss: 2.4710 - val_acc: 0.6300\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.4198 - acc: 0.6409 - val_loss: 2.3684 - val_acc: 0.6380\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3266 - acc: 0.6492 - val_loss: 2.2856 - val_acc: 0.6310\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2512 - acc: 0.6479 - val_loss: 2.2188 - val_acc: 0.6440\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1925 - acc: 0.6475 - val_loss: 2.1662 - val_acc: 0.6490\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1479 - acc: 0.6540 - val_loss: 2.1299 - val_acc: 0.6410\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1143 - acc: 0.6569 - val_loss: 2.0999 - val_acc: 0.6350\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0868 - acc: 0.6551 - val_loss: 2.0739 - val_acc: 0.6510\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0625 - acc: 0.6617 - val_loss: 2.0521 - val_acc: 0.6450\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0403 - acc: 0.6611 - val_loss: 2.0332 - val_acc: 0.6450\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0193 - acc: 0.6608 - val_loss: 2.0123 - val_acc: 0.6630\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9993 - acc: 0.6681 - val_loss: 1.9922 - val_acc: 0.6580\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9800 - acc: 0.6679 - val_loss: 1.9709 - val_acc: 0.6620\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9616 - acc: 0.6715 - val_loss: 1.9558 - val_acc: 0.6600\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9438 - acc: 0.6721 - val_loss: 1.9367 - val_acc: 0.6640\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9262 - acc: 0.6732 - val_loss: 1.9195 - val_acc: 0.6590\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9094 - acc: 0.6756 - val_loss: 1.9029 - val_acc: 0.6670\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8929 - acc: 0.6768 - val_loss: 1.8871 - val_acc: 0.6660\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8770 - acc: 0.6804 - val_loss: 1.8746 - val_acc: 0.6690\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8618 - acc: 0.6787 - val_loss: 1.8583 - val_acc: 0.6730\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8471 - acc: 0.6824 - val_loss: 1.8447 - val_acc: 0.6770\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8328 - acc: 0.6815 - val_loss: 1.8324 - val_acc: 0.6780\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8190 - acc: 0.6829 - val_loss: 1.8161 - val_acc: 0.6790\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8054 - acc: 0.6865 - val_loss: 1.8038 - val_acc: 0.6850\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7921 - acc: 0.6857 - val_loss: 1.7890 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7789 - acc: 0.6872 - val_loss: 1.7783 - val_acc: 0.6880\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7666 - acc: 0.6888 - val_loss: 1.7666 - val_acc: 0.6820\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7541 - acc: 0.6884 - val_loss: 1.7548 - val_acc: 0.6910\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7426 - acc: 0.6912 - val_loss: 1.7421 - val_acc: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7308 - acc: 0.6915 - val_loss: 1.7301 - val_acc: 0.6960\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7195 - acc: 0.6927 - val_loss: 1.7202 - val_acc: 0.6960\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7083 - acc: 0.6937 - val_loss: 1.7100 - val_acc: 0.6890\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6972 - acc: 0.6911 - val_loss: 1.6975 - val_acc: 0.6990\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6866 - acc: 0.6928 - val_loss: 1.6876 - val_acc: 0.6960\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6759 - acc: 0.6937 - val_loss: 1.6791 - val_acc: 0.6990\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6660 - acc: 0.6933 - val_loss: 1.6672 - val_acc: 0.6960\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6558 - acc: 0.6956 - val_loss: 1.6575 - val_acc: 0.7000\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6459 - acc: 0.6963 - val_loss: 1.6478 - val_acc: 0.6980\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6365 - acc: 0.6965 - val_loss: 1.6382 - val_acc: 0.6980\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6270 - acc: 0.6959 - val_loss: 1.6296 - val_acc: 0.6980\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6174 - acc: 0.6985 - val_loss: 1.6225 - val_acc: 0.7020\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6085 - acc: 0.6987 - val_loss: 1.6108 - val_acc: 0.7040\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5992 - acc: 0.6989 - val_loss: 1.6014 - val_acc: 0.7020\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5902 - acc: 0.6999 - val_loss: 1.5944 - val_acc: 0.7030\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5816 - acc: 0.7005 - val_loss: 1.5850 - val_acc: 0.7030\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5732 - acc: 0.7005 - val_loss: 1.5795 - val_acc: 0.7030\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5648 - acc: 0.7015 - val_loss: 1.5749 - val_acc: 0.7090\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5563 - acc: 0.7015 - val_loss: 1.5636 - val_acc: 0.7020\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5483 - acc: 0.7020 - val_loss: 1.5543 - val_acc: 0.7040\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5406 - acc: 0.7032 - val_loss: 1.5469 - val_acc: 0.7040\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5324 - acc: 0.7033 - val_loss: 1.5383 - val_acc: 0.7060\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5245 - acc: 0.7044 - val_loss: 1.5317 - val_acc: 0.7050\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5172 - acc: 0.7055 - val_loss: 1.5234 - val_acc: 0.7100\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5089 - acc: 0.7075 - val_loss: 1.5178 - val_acc: 0.7040\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5019 - acc: 0.7051 - val_loss: 1.5089 - val_acc: 0.7080\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4946 - acc: 0.7067 - val_loss: 1.5006 - val_acc: 0.7050\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4872 - acc: 0.7065 - val_loss: 1.4938 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4797 - acc: 0.7072 - val_loss: 1.4901 - val_acc: 0.7050\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4729 - acc: 0.7080 - val_loss: 1.4814 - val_acc: 0.7080\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4665 - acc: 0.7092 - val_loss: 1.4736 - val_acc: 0.7080\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4589 - acc: 0.7089 - val_loss: 1.4676 - val_acc: 0.7060\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4522 - acc: 0.7096 - val_loss: 1.4633 - val_acc: 0.7060\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4451 - acc: 0.7124 - val_loss: 1.4539 - val_acc: 0.7050\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4388 - acc: 0.7107 - val_loss: 1.4473 - val_acc: 0.7030\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4321 - acc: 0.7129 - val_loss: 1.4396 - val_acc: 0.7110\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4253 - acc: 0.7129 - val_loss: 1.4348 - val_acc: 0.7080\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4191 - acc: 0.7119 - val_loss: 1.4299 - val_acc: 0.7110\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4130 - acc: 0.7127 - val_loss: 1.4261 - val_acc: 0.7090\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4065 - acc: 0.7140 - val_loss: 1.4176 - val_acc: 0.7090\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4004 - acc: 0.7127 - val_loss: 1.4107 - val_acc: 0.7120\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3938 - acc: 0.7132 - val_loss: 1.4045 - val_acc: 0.7090\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3878 - acc: 0.7129 - val_loss: 1.3959 - val_acc: 0.7120\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3816 - acc: 0.7141 - val_loss: 1.3945 - val_acc: 0.7120\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3757 - acc: 0.7144 - val_loss: 1.3868 - val_acc: 0.7170\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3699 - acc: 0.7124 - val_loss: 1.3802 - val_acc: 0.7120\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3640 - acc: 0.7140 - val_loss: 1.3789 - val_acc: 0.7170\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3588 - acc: 0.7160 - val_loss: 1.3678 - val_acc: 0.7110\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3527 - acc: 0.7136 - val_loss: 1.3648 - val_acc: 0.7150\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3474 - acc: 0.7136 - val_loss: 1.3607 - val_acc: 0.7150\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3418 - acc: 0.7163 - val_loss: 1.3535 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3361 - acc: 0.7157 - val_loss: 1.3491 - val_acc: 0.7150\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3309 - acc: 0.7155 - val_loss: 1.3418 - val_acc: 0.7190\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3258 - acc: 0.7151 - val_loss: 1.3381 - val_acc: 0.7140\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3202 - acc: 0.7160 - val_loss: 1.3342 - val_acc: 0.7180\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3150 - acc: 0.7152 - val_loss: 1.3287 - val_acc: 0.7200\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3101 - acc: 0.7149 - val_loss: 1.3224 - val_acc: 0.7170\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3042 - acc: 0.7171 - val_loss: 1.3168 - val_acc: 0.7160\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2994 - acc: 0.7167 - val_loss: 1.3116 - val_acc: 0.7200\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2945 - acc: 0.7167 - val_loss: 1.3109 - val_acc: 0.7200\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2899 - acc: 0.7156 - val_loss: 1.3043 - val_acc: 0.7170\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2852 - acc: 0.7176 - val_loss: 1.2988 - val_acc: 0.7170\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2802 - acc: 0.7172 - val_loss: 1.2973 - val_acc: 0.7160\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2754 - acc: 0.7171 - val_loss: 1.2877 - val_acc: 0.7180\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2711 - acc: 0.7171 - val_loss: 1.2847 - val_acc: 0.7200\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2662 - acc: 0.7179 - val_loss: 1.2841 - val_acc: 0.7180\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2622 - acc: 0.7176 - val_loss: 1.2771 - val_acc: 0.7190\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2573 - acc: 0.7173 - val_loss: 1.2698 - val_acc: 0.7180\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2528 - acc: 0.7191 - val_loss: 1.2679 - val_acc: 0.7200\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2494 - acc: 0.7185 - val_loss: 1.2632 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2444 - acc: 0.7177 - val_loss: 1.2609 - val_acc: 0.7180\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2403 - acc: 0.7185 - val_loss: 1.2549 - val_acc: 0.7200\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2359 - acc: 0.7188 - val_loss: 1.2526 - val_acc: 0.7160\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2319 - acc: 0.7197 - val_loss: 1.2486 - val_acc: 0.7210\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2281 - acc: 0.7220 - val_loss: 1.2446 - val_acc: 0.7220\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2239 - acc: 0.7212 - val_loss: 1.2391 - val_acc: 0.7200\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2196 - acc: 0.7200 - val_loss: 1.2349 - val_acc: 0.7240\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2155 - acc: 0.7195 - val_loss: 1.2344 - val_acc: 0.7200\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2120 - acc: 0.7208 - val_loss: 1.2279 - val_acc: 0.7230\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2083 - acc: 0.7193 - val_loss: 1.2237 - val_acc: 0.7210\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2043 - acc: 0.7209 - val_loss: 1.2205 - val_acc: 0.7220\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2009 - acc: 0.7207 - val_loss: 1.2170 - val_acc: 0.7220\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1976 - acc: 0.7213 - val_loss: 1.2172 - val_acc: 0.7230\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1937 - acc: 0.7217 - val_loss: 1.2118 - val_acc: 0.7210\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1897 - acc: 0.7229 - val_loss: 1.2068 - val_acc: 0.7210\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1868 - acc: 0.7236 - val_loss: 1.2017 - val_acc: 0.7230\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1827 - acc: 0.7220 - val_loss: 1.2071 - val_acc: 0.7220\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1799 - acc: 0.7224 - val_loss: 1.1969 - val_acc: 0.7220\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1761 - acc: 0.7213 - val_loss: 1.1947 - val_acc: 0.7260\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1728 - acc: 0.7220 - val_loss: 1.1940 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1703 - acc: 0.7227 - val_loss: 1.1874 - val_acc: 0.7240\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1667 - acc: 0.7240 - val_loss: 1.1863 - val_acc: 0.7240\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1639 - acc: 0.7223 - val_loss: 1.1837 - val_acc: 0.7300\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1608 - acc: 0.7240 - val_loss: 1.1812 - val_acc: 0.7260\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1574 - acc: 0.7239 - val_loss: 1.1780 - val_acc: 0.7230\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1546 - acc: 0.7245 - val_loss: 1.1736 - val_acc: 0.7260\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1510 - acc: 0.7240 - val_loss: 1.1712 - val_acc: 0.7270\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1484 - acc: 0.7244 - val_loss: 1.1755 - val_acc: 0.7220\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1458 - acc: 0.7233 - val_loss: 1.1689 - val_acc: 0.7250\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1425 - acc: 0.7239 - val_loss: 1.1638 - val_acc: 0.7240\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1401 - acc: 0.7256 - val_loss: 1.1595 - val_acc: 0.7300\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1367 - acc: 0.7263 - val_loss: 1.1561 - val_acc: 0.7260\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1345 - acc: 0.7267 - val_loss: 1.1521 - val_acc: 0.7230\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1315 - acc: 0.7256 - val_loss: 1.1503 - val_acc: 0.7240\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1292 - acc: 0.7253 - val_loss: 1.1479 - val_acc: 0.7260\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1266 - acc: 0.7253 - val_loss: 1.1459 - val_acc: 0.7280\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1243 - acc: 0.7272 - val_loss: 1.1474 - val_acc: 0.7270\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1219 - acc: 0.7277 - val_loss: 1.1445 - val_acc: 0.7290\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1191 - acc: 0.7281 - val_loss: 1.1439 - val_acc: 0.7320\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1171 - acc: 0.7259 - val_loss: 1.1378 - val_acc: 0.7290\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1146 - acc: 0.7276 - val_loss: 1.1369 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1127 - acc: 0.7291 - val_loss: 1.1364 - val_acc: 0.7250\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1104 - acc: 0.7285 - val_loss: 1.1309 - val_acc: 0.7300\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1082 - acc: 0.7295 - val_loss: 1.1343 - val_acc: 0.7310\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1067 - acc: 0.7287 - val_loss: 1.1264 - val_acc: 0.7300\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1037 - acc: 0.7301 - val_loss: 1.1253 - val_acc: 0.7300\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1017 - acc: 0.7300 - val_loss: 1.1253 - val_acc: 0.7300\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1002 - acc: 0.7267 - val_loss: 1.1207 - val_acc: 0.7290\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0977 - acc: 0.7307 - val_loss: 1.1215 - val_acc: 0.7280\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0963 - acc: 0.7297 - val_loss: 1.1228 - val_acc: 0.7320\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0940 - acc: 0.7299 - val_loss: 1.1169 - val_acc: 0.7320\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0918 - acc: 0.7287 - val_loss: 1.1268 - val_acc: 0.7300\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0914 - acc: 0.7280 - val_loss: 1.1198 - val_acc: 0.7330\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0895 - acc: 0.7296 - val_loss: 1.1133 - val_acc: 0.7300\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0875 - acc: 0.7311 - val_loss: 1.1093 - val_acc: 0.7290\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0852 - acc: 0.7313 - val_loss: 1.1089 - val_acc: 0.7330\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0832 - acc: 0.7292 - val_loss: 1.1065 - val_acc: 0.7290\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0815 - acc: 0.7311 - val_loss: 1.1042 - val_acc: 0.7320\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0804 - acc: 0.7319 - val_loss: 1.1029 - val_acc: 0.7280\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0783 - acc: 0.7315 - val_loss: 1.1014 - val_acc: 0.7350\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0770 - acc: 0.7325 - val_loss: 1.0994 - val_acc: 0.7300\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0757 - acc: 0.7299 - val_loss: 1.0985 - val_acc: 0.7260\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0741 - acc: 0.7321 - val_loss: 1.0965 - val_acc: 0.7300\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0722 - acc: 0.7307 - val_loss: 1.0980 - val_acc: 0.7340\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0717 - acc: 0.7325 - val_loss: 1.0983 - val_acc: 0.7330\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0702 - acc: 0.7327 - val_loss: 1.0920 - val_acc: 0.7320\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0679 - acc: 0.7340 - val_loss: 1.0957 - val_acc: 0.7300\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0670 - acc: 0.7317 - val_loss: 1.0898 - val_acc: 0.7310\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0659 - acc: 0.7324 - val_loss: 1.0909 - val_acc: 0.7320\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0644 - acc: 0.7329 - val_loss: 1.0869 - val_acc: 0.7340\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0623 - acc: 0.7343 - val_loss: 1.0870 - val_acc: 0.7330\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0612 - acc: 0.7327 - val_loss: 1.0865 - val_acc: 0.7330\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0597 - acc: 0.7336 - val_loss: 1.0865 - val_acc: 0.7360\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0589 - acc: 0.7320 - val_loss: 1.0808 - val_acc: 0.7370\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0572 - acc: 0.7339 - val_loss: 1.0859 - val_acc: 0.7290\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0563 - acc: 0.7340 - val_loss: 1.0800 - val_acc: 0.7340\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0542 - acc: 0.7341 - val_loss: 1.0822 - val_acc: 0.7300\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0534 - acc: 0.7344 - val_loss: 1.0799 - val_acc: 0.7330\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0524 - acc: 0.7329 - val_loss: 1.0859 - val_acc: 0.7330\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0515 - acc: 0.7329 - val_loss: 1.0753 - val_acc: 0.7340\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0495 - acc: 0.7340 - val_loss: 1.0738 - val_acc: 0.7310\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0486 - acc: 0.7348 - val_loss: 1.0730 - val_acc: 0.7350\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0476 - acc: 0.7355 - val_loss: 1.0721 - val_acc: 0.7340\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0458 - acc: 0.7357 - val_loss: 1.0695 - val_acc: 0.7340\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0448 - acc: 0.7367 - val_loss: 1.0710 - val_acc: 0.7340\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0437 - acc: 0.7359 - val_loss: 1.0763 - val_acc: 0.7380\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0432 - acc: 0.7360 - val_loss: 1.0683 - val_acc: 0.7310\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0412 - acc: 0.7356 - val_loss: 1.0700 - val_acc: 0.7310\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0400 - acc: 0.7368 - val_loss: 1.0713 - val_acc: 0.7290\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0391 - acc: 0.7365 - val_loss: 1.0706 - val_acc: 0.7350\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0373 - acc: 0.7384 - val_loss: 1.0620 - val_acc: 0.7330\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0371 - acc: 0.7369 - val_loss: 1.0645 - val_acc: 0.7380\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0361 - acc: 0.7368 - val_loss: 1.0608 - val_acc: 0.7320\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0341 - acc: 0.7373 - val_loss: 1.0584 - val_acc: 0.7360\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0330 - acc: 0.7356 - val_loss: 1.0583 - val_acc: 0.7370\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0323 - acc: 0.7363 - val_loss: 1.0574 - val_acc: 0.7330\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0314 - acc: 0.7368 - val_loss: 1.0615 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0305 - acc: 0.7367 - val_loss: 1.0593 - val_acc: 0.7300\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0295 - acc: 0.7383 - val_loss: 1.0610 - val_acc: 0.7370\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0285 - acc: 0.7380 - val_loss: 1.0567 - val_acc: 0.7430\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0278 - acc: 0.7367 - val_loss: 1.0556 - val_acc: 0.7400\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0261 - acc: 0.7369 - val_loss: 1.0531 - val_acc: 0.7420\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0249 - acc: 0.7387 - val_loss: 1.0540 - val_acc: 0.7360\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0238 - acc: 0.7397 - val_loss: 1.0512 - val_acc: 0.7360\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0227 - acc: 0.7385 - val_loss: 1.0521 - val_acc: 0.7380\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0213 - acc: 0.7388 - val_loss: 1.0505 - val_acc: 0.7440\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0209 - acc: 0.7388 - val_loss: 1.0487 - val_acc: 0.7380\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0201 - acc: 0.7379 - val_loss: 1.0511 - val_acc: 0.7400\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0188 - acc: 0.7400 - val_loss: 1.0476 - val_acc: 0.7370\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0178 - acc: 0.7383 - val_loss: 1.0505 - val_acc: 0.7380\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0179 - acc: 0.7396 - val_loss: 1.0475 - val_acc: 0.7390\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0168 - acc: 0.7387 - val_loss: 1.0462 - val_acc: 0.7400\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0149 - acc: 0.7405 - val_loss: 1.0439 - val_acc: 0.7430\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0144 - acc: 0.7380 - val_loss: 1.0422 - val_acc: 0.7430\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0133 - acc: 0.7381 - val_loss: 1.0420 - val_acc: 0.7410\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0121 - acc: 0.7413 - val_loss: 1.0419 - val_acc: 0.7410\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0119 - acc: 0.7401 - val_loss: 1.0384 - val_acc: 0.7420\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0104 - acc: 0.7407 - val_loss: 1.0399 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0102 - acc: 0.7403 - val_loss: 1.0387 - val_acc: 0.7410\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0087 - acc: 0.7407 - val_loss: 1.0389 - val_acc: 0.7450\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0074 - acc: 0.7408 - val_loss: 1.0376 - val_acc: 0.7380\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0075 - acc: 0.7401 - val_loss: 1.0355 - val_acc: 0.7430\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0062 - acc: 0.7417 - val_loss: 1.0410 - val_acc: 0.7390\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0060 - acc: 0.7401 - val_loss: 1.0332 - val_acc: 0.7480\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0041 - acc: 0.7431 - val_loss: 1.0343 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0034 - acc: 0.7401 - val_loss: 1.0348 - val_acc: 0.7410\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0028 - acc: 0.7417 - val_loss: 1.0318 - val_acc: 0.7430\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0018 - acc: 0.7416 - val_loss: 1.0322 - val_acc: 0.7410\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0010 - acc: 0.7417 - val_loss: 1.0369 - val_acc: 0.7420\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0006 - acc: 0.7415 - val_loss: 1.0303 - val_acc: 0.7410\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9998 - acc: 0.7419 - val_loss: 1.0302 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9985 - acc: 0.7431 - val_loss: 1.0274 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9977 - acc: 0.7416 - val_loss: 1.0357 - val_acc: 0.7420\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9975 - acc: 0.7407 - val_loss: 1.0290 - val_acc: 0.7390\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9968 - acc: 0.7417 - val_loss: 1.0267 - val_acc: 0.7460\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9955 - acc: 0.7424 - val_loss: 1.0279 - val_acc: 0.7400\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9958 - acc: 0.7424 - val_loss: 1.0260 - val_acc: 0.7420\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9940 - acc: 0.7436 - val_loss: 1.0320 - val_acc: 0.7410\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9936 - acc: 0.7432 - val_loss: 1.0301 - val_acc: 0.7430\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9927 - acc: 0.7432 - val_loss: 1.0291 - val_acc: 0.7410\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9922 - acc: 0.7447 - val_loss: 1.0226 - val_acc: 0.7480\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9906 - acc: 0.7456 - val_loss: 1.0237 - val_acc: 0.7440\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9908 - acc: 0.7423 - val_loss: 1.0213 - val_acc: 0.7500\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9896 - acc: 0.7425 - val_loss: 1.0221 - val_acc: 0.7440\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9895 - acc: 0.7451 - val_loss: 1.0213 - val_acc: 0.7450\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9880 - acc: 0.7445 - val_loss: 1.0210 - val_acc: 0.7460\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9872 - acc: 0.7444 - val_loss: 1.0224 - val_acc: 0.7420\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9866 - acc: 0.7448 - val_loss: 1.0180 - val_acc: 0.7460\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9863 - acc: 0.7439 - val_loss: 1.0174 - val_acc: 0.7500\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9858 - acc: 0.7437 - val_loss: 1.0250 - val_acc: 0.7410\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9854 - acc: 0.7453 - val_loss: 1.0239 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9842 - acc: 0.7440 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9837 - acc: 0.7460 - val_loss: 1.0173 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9826 - acc: 0.7453 - val_loss: 1.0140 - val_acc: 0.7520\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9820 - acc: 0.7433 - val_loss: 1.0146 - val_acc: 0.7470\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9810 - acc: 0.7448 - val_loss: 1.0174 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9807 - acc: 0.7431 - val_loss: 1.0139 - val_acc: 0.7470\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9799 - acc: 0.7468 - val_loss: 1.0132 - val_acc: 0.7510\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9789 - acc: 0.7451 - val_loss: 1.0122 - val_acc: 0.7510\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9785 - acc: 0.7457 - val_loss: 1.0148 - val_acc: 0.7470\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9776 - acc: 0.7477 - val_loss: 1.0108 - val_acc: 0.7520\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9771 - acc: 0.7457 - val_loss: 1.0095 - val_acc: 0.7530\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9767 - acc: 0.7468 - val_loss: 1.0121 - val_acc: 0.7450\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9762 - acc: 0.7479 - val_loss: 1.0103 - val_acc: 0.7550\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9755 - acc: 0.7487 - val_loss: 1.0106 - val_acc: 0.7460\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9750 - acc: 0.7472 - val_loss: 1.0081 - val_acc: 0.7500\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9746 - acc: 0.7464 - val_loss: 1.0121 - val_acc: 0.7460\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9731 - acc: 0.7477 - val_loss: 1.0091 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9723 - acc: 0.7460 - val_loss: 1.0071 - val_acc: 0.7520\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9719 - acc: 0.7471 - val_loss: 1.0085 - val_acc: 0.7520\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9719 - acc: 0.7475 - val_loss: 1.0093 - val_acc: 0.7480\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9707 - acc: 0.7479 - val_loss: 1.0093 - val_acc: 0.7540\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9703 - acc: 0.7467 - val_loss: 1.0077 - val_acc: 0.7480\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9694 - acc: 0.7468 - val_loss: 1.0150 - val_acc: 0.7460\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9693 - acc: 0.7480 - val_loss: 1.0053 - val_acc: 0.7520\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9683 - acc: 0.7475 - val_loss: 1.0058 - val_acc: 0.7490\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9675 - acc: 0.7469 - val_loss: 1.0032 - val_acc: 0.7500\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9663 - acc: 0.7483 - val_loss: 1.0072 - val_acc: 0.7500\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9669 - acc: 0.7481 - val_loss: 1.0028 - val_acc: 0.7510\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9661 - acc: 0.7503 - val_loss: 1.0012 - val_acc: 0.7530\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9651 - acc: 0.7495 - val_loss: 1.0008 - val_acc: 0.7520\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9645 - acc: 0.7496 - val_loss: 1.0050 - val_acc: 0.7510\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9650 - acc: 0.7483 - val_loss: 1.0014 - val_acc: 0.7490\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9633 - acc: 0.7508 - val_loss: 1.0020 - val_acc: 0.7510\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9625 - acc: 0.7493 - val_loss: 1.0020 - val_acc: 0.7500\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9623 - acc: 0.7497 - val_loss: 0.9982 - val_acc: 0.7530\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9613 - acc: 0.7477 - val_loss: 1.0024 - val_acc: 0.7500\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9617 - acc: 0.7497 - val_loss: 1.0051 - val_acc: 0.7430\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9606 - acc: 0.7496 - val_loss: 0.9980 - val_acc: 0.7550\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9601 - acc: 0.7496 - val_loss: 1.0006 - val_acc: 0.7550\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9600 - acc: 0.7505 - val_loss: 1.0005 - val_acc: 0.7540\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9586 - acc: 0.7515 - val_loss: 1.0028 - val_acc: 0.7460\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9588 - acc: 0.7497 - val_loss: 0.9968 - val_acc: 0.7540\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9582 - acc: 0.7516 - val_loss: 0.9962 - val_acc: 0.7530\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9580 - acc: 0.7512 - val_loss: 1.0024 - val_acc: 0.7480\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9573 - acc: 0.7499 - val_loss: 0.9967 - val_acc: 0.7470\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9557 - acc: 0.7495 - val_loss: 0.9951 - val_acc: 0.7540\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9554 - acc: 0.7505 - val_loss: 0.9930 - val_acc: 0.7530\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9545 - acc: 0.7516 - val_loss: 0.9939 - val_acc: 0.7510\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9545 - acc: 0.7511 - val_loss: 0.9965 - val_acc: 0.7490\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9543 - acc: 0.7512 - val_loss: 0.9935 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9532 - acc: 0.7501 - val_loss: 0.9903 - val_acc: 0.7530\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9529 - acc: 0.7493 - val_loss: 0.9916 - val_acc: 0.7550\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9523 - acc: 0.7512 - val_loss: 0.9907 - val_acc: 0.7510\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9515 - acc: 0.7499 - val_loss: 0.9916 - val_acc: 0.7550\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9514 - acc: 0.7524 - val_loss: 0.9896 - val_acc: 0.7560\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9506 - acc: 0.7507 - val_loss: 0.9906 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9505 - acc: 0.7521 - val_loss: 0.9894 - val_acc: 0.7510\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9499 - acc: 0.7529 - val_loss: 0.9901 - val_acc: 0.7520\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9489 - acc: 0.7504 - val_loss: 0.9893 - val_acc: 0.7560\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9491 - acc: 0.7528 - val_loss: 0.9916 - val_acc: 0.7570\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9487 - acc: 0.7501 - val_loss: 0.9881 - val_acc: 0.7530\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9478 - acc: 0.7517 - val_loss: 0.9854 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9479 - acc: 0.7520 - val_loss: 0.9859 - val_acc: 0.7550\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9466 - acc: 0.7523 - val_loss: 0.9928 - val_acc: 0.7520\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9466 - acc: 0.7505 - val_loss: 0.9904 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9458 - acc: 0.7519 - val_loss: 0.9833 - val_acc: 0.7560\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9454 - acc: 0.7520 - val_loss: 0.9844 - val_acc: 0.7540\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9446 - acc: 0.7521 - val_loss: 0.9855 - val_acc: 0.7570\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9440 - acc: 0.7547 - val_loss: 0.9830 - val_acc: 0.7570\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9438 - acc: 0.7529 - val_loss: 0.9962 - val_acc: 0.7510\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9438 - acc: 0.7521 - val_loss: 0.9936 - val_acc: 0.7530\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9438 - acc: 0.7525 - val_loss: 0.9847 - val_acc: 0.7530\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9421 - acc: 0.7515 - val_loss: 0.9824 - val_acc: 0.7580\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9412 - acc: 0.7521 - val_loss: 0.9815 - val_acc: 0.7540\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9408 - acc: 0.7537 - val_loss: 0.9882 - val_acc: 0.7510\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9415 - acc: 0.7523 - val_loss: 0.9885 - val_acc: 0.7580\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9401 - acc: 0.7540 - val_loss: 0.9837 - val_acc: 0.7520\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9404 - acc: 0.7533 - val_loss: 0.9907 - val_acc: 0.7490\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9399 - acc: 0.7524 - val_loss: 0.9780 - val_acc: 0.7550\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9384 - acc: 0.7529 - val_loss: 0.9796 - val_acc: 0.7550\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9385 - acc: 0.7536 - val_loss: 0.9863 - val_acc: 0.7520\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9383 - acc: 0.7539 - val_loss: 0.9814 - val_acc: 0.7530\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9373 - acc: 0.7547 - val_loss: 0.9792 - val_acc: 0.7560\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9367 - acc: 0.7543 - val_loss: 0.9819 - val_acc: 0.7510\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9373 - acc: 0.7535 - val_loss: 0.9802 - val_acc: 0.7500\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9358 - acc: 0.7529 - val_loss: 0.9812 - val_acc: 0.7560\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9364 - acc: 0.7557 - val_loss: 0.9808 - val_acc: 0.7610\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9354 - acc: 0.7548 - val_loss: 0.9764 - val_acc: 0.7560\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9345 - acc: 0.7556 - val_loss: 0.9786 - val_acc: 0.7590\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9347 - acc: 0.7537 - val_loss: 0.9791 - val_acc: 0.7550\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9349 - acc: 0.7524 - val_loss: 0.9786 - val_acc: 0.7570\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9338 - acc: 0.7547 - val_loss: 0.9867 - val_acc: 0.7590\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9337 - acc: 0.7539 - val_loss: 0.9753 - val_acc: 0.7520\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9328 - acc: 0.7545 - val_loss: 0.9768 - val_acc: 0.7560\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9323 - acc: 0.7552 - val_loss: 0.9811 - val_acc: 0.7530\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9326 - acc: 0.7551 - val_loss: 0.9787 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9315 - acc: 0.7540 - val_loss: 0.9777 - val_acc: 0.7550\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9314 - acc: 0.7551 - val_loss: 0.9755 - val_acc: 0.7530\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9305 - acc: 0.7551 - val_loss: 0.9730 - val_acc: 0.7570\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9298 - acc: 0.7551 - val_loss: 0.9728 - val_acc: 0.7510\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9295 - acc: 0.7568 - val_loss: 0.9735 - val_acc: 0.7580\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9293 - acc: 0.7575 - val_loss: 0.9736 - val_acc: 0.7580\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9288 - acc: 0.7553 - val_loss: 0.9731 - val_acc: 0.7520\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9288 - acc: 0.7559 - val_loss: 0.9733 - val_acc: 0.7570\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9293 - acc: 0.7553 - val_loss: 0.9736 - val_acc: 0.7600\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9284 - acc: 0.7545 - val_loss: 0.9729 - val_acc: 0.7610\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9281 - acc: 0.7557 - val_loss: 0.9709 - val_acc: 0.7610\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9272 - acc: 0.7548 - val_loss: 0.9687 - val_acc: 0.7560\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9265 - acc: 0.7568 - val_loss: 0.9751 - val_acc: 0.7550\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9266 - acc: 0.7577 - val_loss: 0.9752 - val_acc: 0.7540\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9251 - acc: 0.7571 - val_loss: 0.9697 - val_acc: 0.7550\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9255 - acc: 0.7551 - val_loss: 0.9732 - val_acc: 0.7530\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9260 - acc: 0.7564 - val_loss: 0.9705 - val_acc: 0.7570\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9249 - acc: 0.7557 - val_loss: 0.9682 - val_acc: 0.7590\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9243 - acc: 0.7555 - val_loss: 0.9727 - val_acc: 0.7590\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9248 - acc: 0.7579 - val_loss: 0.9670 - val_acc: 0.7560\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9237 - acc: 0.7576 - val_loss: 0.9662 - val_acc: 0.7600\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9232 - acc: 0.7573 - val_loss: 0.9674 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9232 - acc: 0.7561 - val_loss: 0.9701 - val_acc: 0.7560\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9225 - acc: 0.7552 - val_loss: 0.9678 - val_acc: 0.7560\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9229 - acc: 0.7573 - val_loss: 0.9651 - val_acc: 0.7630\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9215 - acc: 0.7569 - val_loss: 0.9770 - val_acc: 0.7610\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9222 - acc: 0.7551 - val_loss: 0.9654 - val_acc: 0.7600\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9213 - acc: 0.7584 - val_loss: 0.9686 - val_acc: 0.7520\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9213 - acc: 0.7583 - val_loss: 0.9653 - val_acc: 0.7600\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9202 - acc: 0.7573 - val_loss: 0.9733 - val_acc: 0.7520\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9207 - acc: 0.7569 - val_loss: 0.9661 - val_acc: 0.7560\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9202 - acc: 0.7563 - val_loss: 0.9651 - val_acc: 0.7570\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9187 - acc: 0.7577 - val_loss: 0.9652 - val_acc: 0.7540\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9180 - acc: 0.7571 - val_loss: 0.9684 - val_acc: 0.7550\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9189 - acc: 0.7571 - val_loss: 0.9727 - val_acc: 0.7610\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9182 - acc: 0.7581 - val_loss: 0.9665 - val_acc: 0.7500\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9183 - acc: 0.7559 - val_loss: 0.9619 - val_acc: 0.7570\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9173 - acc: 0.7568 - val_loss: 0.9657 - val_acc: 0.7550\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9174 - acc: 0.7559 - val_loss: 0.9674 - val_acc: 0.7570\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9169 - acc: 0.7588 - val_loss: 0.9685 - val_acc: 0.7570\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9169 - acc: 0.7548 - val_loss: 0.9624 - val_acc: 0.7530\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9163 - acc: 0.7580 - val_loss: 0.9649 - val_acc: 0.7580\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9154 - acc: 0.7579 - val_loss: 0.9655 - val_acc: 0.7570\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9156 - acc: 0.7568 - val_loss: 0.9584 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9141 - acc: 0.7573 - val_loss: 0.9615 - val_acc: 0.7570\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9150 - acc: 0.7573 - val_loss: 0.9625 - val_acc: 0.7600\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9144 - acc: 0.7584 - val_loss: 0.9648 - val_acc: 0.7570\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9150 - acc: 0.7587 - val_loss: 0.9607 - val_acc: 0.7560\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9130 - acc: 0.7585 - val_loss: 0.9616 - val_acc: 0.7550\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9142 - acc: 0.7567 - val_loss: 0.9631 - val_acc: 0.7560\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9128 - acc: 0.7580 - val_loss: 0.9593 - val_acc: 0.7550\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9126 - acc: 0.7583 - val_loss: 0.9651 - val_acc: 0.7550\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9120 - acc: 0.7592 - val_loss: 0.9632 - val_acc: 0.7600\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9129 - acc: 0.7592 - val_loss: 0.9584 - val_acc: 0.7570\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9118 - acc: 0.7581 - val_loss: 0.9633 - val_acc: 0.7530\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9111 - acc: 0.7585 - val_loss: 0.9564 - val_acc: 0.7580\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9104 - acc: 0.7600 - val_loss: 0.9563 - val_acc: 0.7580\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9099 - acc: 0.7583 - val_loss: 0.9560 - val_acc: 0.7580\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9111 - acc: 0.7596 - val_loss: 0.9576 - val_acc: 0.7530\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9097 - acc: 0.7583 - val_loss: 0.9628 - val_acc: 0.7530\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9094 - acc: 0.7595 - val_loss: 0.9603 - val_acc: 0.7540\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9096 - acc: 0.7597 - val_loss: 0.9547 - val_acc: 0.7600\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9084 - acc: 0.7592 - val_loss: 0.9558 - val_acc: 0.7580\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9080 - acc: 0.7593 - val_loss: 0.9686 - val_acc: 0.7580\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9086 - acc: 0.7580 - val_loss: 0.9553 - val_acc: 0.7610\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9080 - acc: 0.7593 - val_loss: 0.9574 - val_acc: 0.7580\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9079 - acc: 0.7591 - val_loss: 0.9559 - val_acc: 0.7590\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9079 - acc: 0.7591 - val_loss: 0.9571 - val_acc: 0.7610\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9073 - acc: 0.7584 - val_loss: 0.9555 - val_acc: 0.7550\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9072 - acc: 0.7599 - val_loss: 0.9527 - val_acc: 0.7570\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7603 - val_loss: 0.9601 - val_acc: 0.7540\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9063 - acc: 0.7592 - val_loss: 0.9597 - val_acc: 0.7580\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9056 - acc: 0.7601 - val_loss: 0.9529 - val_acc: 0.7600\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9054 - acc: 0.7595 - val_loss: 0.9550 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7608 - val_loss: 0.9539 - val_acc: 0.7590\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9040 - acc: 0.7600 - val_loss: 0.9510 - val_acc: 0.7600\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9045 - acc: 0.7577 - val_loss: 0.9601 - val_acc: 0.7560\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9043 - acc: 0.7607 - val_loss: 0.9520 - val_acc: 0.7580\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9046 - acc: 0.7608 - val_loss: 0.9532 - val_acc: 0.7540\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9031 - acc: 0.7591 - val_loss: 0.9518 - val_acc: 0.7630\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9029 - acc: 0.7605 - val_loss: 0.9560 - val_acc: 0.7550\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9028 - acc: 0.7612 - val_loss: 0.9532 - val_acc: 0.7590\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9024 - acc: 0.7591 - val_loss: 0.9498 - val_acc: 0.7590\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7615 - val_loss: 0.9489 - val_acc: 0.7580\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9007 - acc: 0.7611 - val_loss: 0.9544 - val_acc: 0.7530\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9017 - acc: 0.7605 - val_loss: 0.9507 - val_acc: 0.7580\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9017 - acc: 0.7611 - val_loss: 0.9541 - val_acc: 0.7540\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9009 - acc: 0.7601 - val_loss: 0.9513 - val_acc: 0.7600\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9015 - acc: 0.7604 - val_loss: 0.9566 - val_acc: 0.7540\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8996 - acc: 0.7633 - val_loss: 0.9528 - val_acc: 0.7580\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9012 - acc: 0.7609 - val_loss: 0.9463 - val_acc: 0.7590\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8993 - acc: 0.7600 - val_loss: 0.9473 - val_acc: 0.7580\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8994 - acc: 0.7612 - val_loss: 0.9511 - val_acc: 0.7550\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9004 - acc: 0.7608 - val_loss: 0.9509 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8984 - acc: 0.7613 - val_loss: 0.9510 - val_acc: 0.7550\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8983 - acc: 0.7615 - val_loss: 0.9466 - val_acc: 0.7590\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8972 - acc: 0.7636 - val_loss: 0.9480 - val_acc: 0.7600\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7607 - val_loss: 0.9572 - val_acc: 0.7620\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8976 - acc: 0.7607 - val_loss: 0.9457 - val_acc: 0.7570\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8960 - acc: 0.7620 - val_loss: 0.9462 - val_acc: 0.7560\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8975 - acc: 0.7615 - val_loss: 0.9481 - val_acc: 0.7560\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8967 - acc: 0.7611 - val_loss: 0.9579 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8963 - acc: 0.7613 - val_loss: 0.9451 - val_acc: 0.7560\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8960 - acc: 0.7604 - val_loss: 0.9453 - val_acc: 0.7610\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8951 - acc: 0.7631 - val_loss: 0.9454 - val_acc: 0.7550\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8953 - acc: 0.7616 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8943 - acc: 0.7628 - val_loss: 0.9419 - val_acc: 0.7590\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8933 - acc: 0.7633 - val_loss: 0.9477 - val_acc: 0.7550\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8948 - acc: 0.7605 - val_loss: 0.9432 - val_acc: 0.7610\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8935 - acc: 0.7628 - val_loss: 0.9555 - val_acc: 0.7560\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8938 - acc: 0.7629 - val_loss: 0.9434 - val_acc: 0.7580\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8933 - acc: 0.7617 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8935 - acc: 0.7609 - val_loss: 0.9494 - val_acc: 0.7550\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8924 - acc: 0.7621 - val_loss: 0.9400 - val_acc: 0.7590\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8926 - acc: 0.7604 - val_loss: 0.9444 - val_acc: 0.7620\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8915 - acc: 0.7619 - val_loss: 0.9470 - val_acc: 0.7550\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8924 - acc: 0.7599 - val_loss: 0.9389 - val_acc: 0.7580\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8904 - acc: 0.7632 - val_loss: 0.9445 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8910 - acc: 0.7629 - val_loss: 0.9453 - val_acc: 0.7610\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8901 - acc: 0.7637 - val_loss: 0.9398 - val_acc: 0.7610\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8900 - acc: 0.7629 - val_loss: 0.9413 - val_acc: 0.7540\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8892 - acc: 0.7623 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8892 - acc: 0.7632 - val_loss: 0.9423 - val_acc: 0.7620\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8895 - acc: 0.7635 - val_loss: 0.9676 - val_acc: 0.7500\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8904 - acc: 0.7644 - val_loss: 0.9385 - val_acc: 0.7610\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8882 - acc: 0.7627 - val_loss: 0.9483 - val_acc: 0.7550\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8892 - acc: 0.7627 - val_loss: 0.9429 - val_acc: 0.7540\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8889 - acc: 0.7635 - val_loss: 0.9412 - val_acc: 0.7580\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8883 - acc: 0.7624 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8874 - acc: 0.7631 - val_loss: 0.9391 - val_acc: 0.7560\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8878 - acc: 0.7616 - val_loss: 0.9413 - val_acc: 0.7590\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8863 - acc: 0.7633 - val_loss: 0.9458 - val_acc: 0.7580\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8877 - acc: 0.7647 - val_loss: 0.9723 - val_acc: 0.7560\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8881 - acc: 0.7656 - val_loss: 0.9410 - val_acc: 0.7620\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8861 - acc: 0.7639 - val_loss: 0.9364 - val_acc: 0.7580\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8856 - acc: 0.7639 - val_loss: 0.9399 - val_acc: 0.7550\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8858 - acc: 0.7645 - val_loss: 0.9383 - val_acc: 0.7600\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8848 - acc: 0.7641 - val_loss: 0.9354 - val_acc: 0.7580\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7629 - val_loss: 0.9407 - val_acc: 0.7540\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8852 - acc: 0.7648 - val_loss: 0.9354 - val_acc: 0.7630\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8838 - acc: 0.7660 - val_loss: 0.9385 - val_acc: 0.7620\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8844 - acc: 0.7621 - val_loss: 0.9385 - val_acc: 0.7570\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8840 - acc: 0.7648 - val_loss: 0.9354 - val_acc: 0.7600\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8828 - acc: 0.7648 - val_loss: 0.9410 - val_acc: 0.7540\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8841 - acc: 0.7660 - val_loss: 0.9386 - val_acc: 0.7600\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8829 - acc: 0.7661 - val_loss: 0.9395 - val_acc: 0.7600\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8821 - acc: 0.7645 - val_loss: 0.9335 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8823 - acc: 0.7639 - val_loss: 0.9380 - val_acc: 0.7570\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8817 - acc: 0.7635 - val_loss: 0.9384 - val_acc: 0.7560\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8823 - acc: 0.7648 - val_loss: 0.9342 - val_acc: 0.7610\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8810 - acc: 0.7641 - val_loss: 0.9330 - val_acc: 0.7600\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8808 - acc: 0.7644 - val_loss: 0.9350 - val_acc: 0.7600\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8802 - acc: 0.7632 - val_loss: 0.9347 - val_acc: 0.7630\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8813 - acc: 0.7643 - val_loss: 0.9388 - val_acc: 0.7610\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8808 - acc: 0.7645 - val_loss: 0.9395 - val_acc: 0.7570\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8812 - acc: 0.7648 - val_loss: 0.9376 - val_acc: 0.7550\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8797 - acc: 0.7652 - val_loss: 0.9339 - val_acc: 0.7570\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8800 - acc: 0.7653 - val_loss: 0.9339 - val_acc: 0.7620\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8789 - acc: 0.7676 - val_loss: 0.9308 - val_acc: 0.7570\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8789 - acc: 0.7651 - val_loss: 0.9334 - val_acc: 0.7580\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8795 - acc: 0.7653 - val_loss: 0.9318 - val_acc: 0.7580\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8780 - acc: 0.7669 - val_loss: 0.9325 - val_acc: 0.7630\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8778 - acc: 0.7668 - val_loss: 0.9306 - val_acc: 0.7640\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8770 - acc: 0.7656 - val_loss: 0.9378 - val_acc: 0.7600\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8776 - acc: 0.7649 - val_loss: 0.9377 - val_acc: 0.7600\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8781 - acc: 0.7655 - val_loss: 0.9354 - val_acc: 0.7550\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8765 - acc: 0.7664 - val_loss: 0.9311 - val_acc: 0.7610\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8765 - acc: 0.7649 - val_loss: 0.9332 - val_acc: 0.7590\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8759 - acc: 0.7664 - val_loss: 0.9361 - val_acc: 0.7620\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8765 - acc: 0.7675 - val_loss: 0.9295 - val_acc: 0.7650\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8753 - acc: 0.7660 - val_loss: 0.9292 - val_acc: 0.7620\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8746 - acc: 0.7679 - val_loss: 0.9332 - val_acc: 0.7570\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8752 - acc: 0.7665 - val_loss: 0.9382 - val_acc: 0.7600\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8758 - acc: 0.7669 - val_loss: 0.9408 - val_acc: 0.7580\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8765 - acc: 0.7653 - val_loss: 0.9307 - val_acc: 0.7610\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8728 - acc: 0.7653 - val_loss: 0.9273 - val_acc: 0.7630\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8731 - acc: 0.7668 - val_loss: 0.9356 - val_acc: 0.7580\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8736 - acc: 0.7655 - val_loss: 0.9373 - val_acc: 0.7660\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8740 - acc: 0.7680 - val_loss: 0.9282 - val_acc: 0.7610\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8735 - acc: 0.7660 - val_loss: 0.9297 - val_acc: 0.7580\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8730 - acc: 0.7651 - val_loss: 0.9264 - val_acc: 0.7600\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8713 - acc: 0.7669 - val_loss: 0.9310 - val_acc: 0.7620\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8721 - acc: 0.7664 - val_loss: 0.9274 - val_acc: 0.7620\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8714 - acc: 0.7661 - val_loss: 0.9275 - val_acc: 0.7660\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8716 - acc: 0.7668 - val_loss: 0.9244 - val_acc: 0.7570\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8705 - acc: 0.7679 - val_loss: 0.9248 - val_acc: 0.7590\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8715 - acc: 0.7659 - val_loss: 0.9275 - val_acc: 0.7600\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8712 - acc: 0.7671 - val_loss: 0.9266 - val_acc: 0.7590\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8709 - acc: 0.7671 - val_loss: 0.9248 - val_acc: 0.7610\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8691 - acc: 0.7688 - val_loss: 0.9438 - val_acc: 0.7560\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8702 - acc: 0.7685 - val_loss: 0.9308 - val_acc: 0.7550\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8691 - acc: 0.7684 - val_loss: 0.9381 - val_acc: 0.7520\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8689 - acc: 0.7684 - val_loss: 0.9335 - val_acc: 0.7580\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8697 - acc: 0.7693 - val_loss: 0.9230 - val_acc: 0.7610\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8694 - acc: 0.7672 - val_loss: 0.9302 - val_acc: 0.7690\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8686 - acc: 0.7669 - val_loss: 0.9474 - val_acc: 0.7560\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8697 - acc: 0.7669 - val_loss: 0.9251 - val_acc: 0.7590\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8682 - acc: 0.7687 - val_loss: 0.9342 - val_acc: 0.7640\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8684 - acc: 0.7680 - val_loss: 0.9216 - val_acc: 0.7640\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8670 - acc: 0.7652 - val_loss: 0.9535 - val_acc: 0.7510\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8685 - acc: 0.7672 - val_loss: 0.9268 - val_acc: 0.7610\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8671 - acc: 0.7692 - val_loss: 0.9266 - val_acc: 0.7570\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8677 - acc: 0.7676 - val_loss: 0.9370 - val_acc: 0.7520\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8674 - acc: 0.7676 - val_loss: 0.9270 - val_acc: 0.7580\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8664 - acc: 0.7644 - val_loss: 0.9231 - val_acc: 0.7620\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8654 - acc: 0.7701 - val_loss: 0.9212 - val_acc: 0.7610\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8666 - acc: 0.7688 - val_loss: 0.9232 - val_acc: 0.7580\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8652 - acc: 0.7677 - val_loss: 0.9272 - val_acc: 0.7620\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8647 - acc: 0.7700 - val_loss: 0.9219 - val_acc: 0.7610\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8648 - acc: 0.7699 - val_loss: 0.9386 - val_acc: 0.7600\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8655 - acc: 0.7700 - val_loss: 0.9199 - val_acc: 0.7610\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8646 - acc: 0.7704 - val_loss: 0.9235 - val_acc: 0.7610\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8646 - acc: 0.7705 - val_loss: 0.9275 - val_acc: 0.7600\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8641 - acc: 0.7684 - val_loss: 0.9243 - val_acc: 0.7600\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8643 - acc: 0.7700 - val_loss: 0.9226 - val_acc: 0.7610\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8630 - acc: 0.7683 - val_loss: 0.9481 - val_acc: 0.7510\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8643 - acc: 0.7704 - val_loss: 0.9276 - val_acc: 0.7670\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8635 - acc: 0.7695 - val_loss: 0.9239 - val_acc: 0.7630\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8627 - acc: 0.7696 - val_loss: 0.9231 - val_acc: 0.7600\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8634 - acc: 0.7711 - val_loss: 0.9194 - val_acc: 0.7630\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8622 - acc: 0.7697 - val_loss: 0.9281 - val_acc: 0.7600\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8616 - acc: 0.7700 - val_loss: 0.9315 - val_acc: 0.7540\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8626 - acc: 0.7716 - val_loss: 0.9247 - val_acc: 0.7580\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8626 - acc: 0.7691 - val_loss: 0.9196 - val_acc: 0.7630\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8615 - acc: 0.7696 - val_loss: 0.9194 - val_acc: 0.7630\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8611 - acc: 0.7695 - val_loss: 0.9205 - val_acc: 0.7580\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8614 - acc: 0.7689 - val_loss: 0.9190 - val_acc: 0.7600\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8620 - acc: 0.7685 - val_loss: 0.9231 - val_acc: 0.7580\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8608 - acc: 0.7727 - val_loss: 0.9201 - val_acc: 0.7600\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8608 - acc: 0.7699 - val_loss: 0.9227 - val_acc: 0.7620\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8599 - acc: 0.7703 - val_loss: 0.9200 - val_acc: 0.7610\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8598 - acc: 0.7696 - val_loss: 0.9212 - val_acc: 0.7630\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8590 - acc: 0.7708 - val_loss: 0.9276 - val_acc: 0.7610\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8592 - acc: 0.7703 - val_loss: 0.9197 - val_acc: 0.7630\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8582 - acc: 0.7727 - val_loss: 0.9265 - val_acc: 0.7640\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8585 - acc: 0.7731 - val_loss: 0.9213 - val_acc: 0.7610\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8574 - acc: 0.7721 - val_loss: 0.9253 - val_acc: 0.7590\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8586 - acc: 0.7707 - val_loss: 0.9377 - val_acc: 0.7570\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8595 - acc: 0.7707 - val_loss: 0.9154 - val_acc: 0.7600\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8579 - acc: 0.7715 - val_loss: 0.9218 - val_acc: 0.7590\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8574 - acc: 0.7704 - val_loss: 0.9212 - val_acc: 0.7570\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8563 - acc: 0.7732 - val_loss: 0.9401 - val_acc: 0.7650\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8582 - acc: 0.7701 - val_loss: 0.9231 - val_acc: 0.7670\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8576 - acc: 0.7699 - val_loss: 0.9195 - val_acc: 0.7620\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8567 - acc: 0.7720 - val_loss: 0.9409 - val_acc: 0.7520\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8577 - acc: 0.7708 - val_loss: 0.9167 - val_acc: 0.7610\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8557 - acc: 0.7696 - val_loss: 0.9197 - val_acc: 0.7630\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8573 - acc: 0.7720 - val_loss: 0.9417 - val_acc: 0.7490\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8572 - acc: 0.7704 - val_loss: 0.9250 - val_acc: 0.7620\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8551 - acc: 0.7735 - val_loss: 0.9229 - val_acc: 0.7620\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8552 - acc: 0.7721 - val_loss: 0.9286 - val_acc: 0.7590\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8552 - acc: 0.7721 - val_loss: 0.9279 - val_acc: 0.7610\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8555 - acc: 0.7723 - val_loss: 0.9209 - val_acc: 0.7660\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8545 - acc: 0.7735 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8542 - acc: 0.7729 - val_loss: 0.9235 - val_acc: 0.7570\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8557 - acc: 0.7713 - val_loss: 0.9164 - val_acc: 0.7610\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.7721 - val_loss: 0.9166 - val_acc: 0.7700\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8547 - acc: 0.7724 - val_loss: 0.9208 - val_acc: 0.7620\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8538 - acc: 0.7723 - val_loss: 0.9172 - val_acc: 0.7610\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8531 - acc: 0.7731 - val_loss: 0.9200 - val_acc: 0.7680\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8517 - acc: 0.7732 - val_loss: 0.9155 - val_acc: 0.7630\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8530 - acc: 0.7716 - val_loss: 0.9395 - val_acc: 0.7640\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8546 - acc: 0.7733 - val_loss: 0.9267 - val_acc: 0.7600\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8528 - acc: 0.7719 - val_loss: 0.9188 - val_acc: 0.7580\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8523 - acc: 0.7719 - val_loss: 0.9246 - val_acc: 0.7550\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8533 - acc: 0.7717 - val_loss: 0.9211 - val_acc: 0.7590\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8512 - acc: 0.7744 - val_loss: 0.9269 - val_acc: 0.7620\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8520 - acc: 0.7716 - val_loss: 0.9166 - val_acc: 0.7630\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.7717 - val_loss: 0.9134 - val_acc: 0.7620\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8501 - acc: 0.7727 - val_loss: 0.9183 - val_acc: 0.7600\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8511 - acc: 0.7757 - val_loss: 0.9188 - val_acc: 0.7610\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8505 - acc: 0.7759 - val_loss: 0.9104 - val_acc: 0.7610\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8498 - acc: 0.7741 - val_loss: 0.9119 - val_acc: 0.7640\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.7756 - val_loss: 0.9152 - val_acc: 0.7590\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8500 - acc: 0.7720 - val_loss: 0.9236 - val_acc: 0.7680\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8497 - acc: 0.7737 - val_loss: 0.9409 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8505 - acc: 0.7741 - val_loss: 0.9170 - val_acc: 0.7680\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8491 - acc: 0.7727 - val_loss: 0.9161 - val_acc: 0.7690\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8495 - acc: 0.7756 - val_loss: 0.9214 - val_acc: 0.7690\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.7760 - val_loss: 0.9128 - val_acc: 0.7650\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8497 - acc: 0.7731 - val_loss: 0.9159 - val_acc: 0.7720\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8479 - acc: 0.7747 - val_loss: 0.9190 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8486 - acc: 0.7736 - val_loss: 0.9413 - val_acc: 0.7580\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8488 - acc: 0.7745 - val_loss: 0.9214 - val_acc: 0.7670\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8485 - acc: 0.7723 - val_loss: 0.9193 - val_acc: 0.7600\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8476 - acc: 0.7745 - val_loss: 0.9243 - val_acc: 0.7580\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8484 - acc: 0.7731 - val_loss: 0.9180 - val_acc: 0.7660\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8464 - acc: 0.7748 - val_loss: 0.9237 - val_acc: 0.7590\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8483 - acc: 0.7767 - val_loss: 0.9119 - val_acc: 0.7640\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.7735 - val_loss: 0.9313 - val_acc: 0.7530\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8483 - acc: 0.7751 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8463 - acc: 0.7747 - val_loss: 0.9096 - val_acc: 0.7610\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8456 - acc: 0.7739 - val_loss: 0.9183 - val_acc: 0.7650\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8456 - acc: 0.7764 - val_loss: 0.9148 - val_acc: 0.7600\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8465 - acc: 0.7731 - val_loss: 0.9191 - val_acc: 0.7590\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8478 - acc: 0.7735 - val_loss: 0.9123 - val_acc: 0.7620\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8467 - acc: 0.7732 - val_loss: 0.9117 - val_acc: 0.7680\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8437 - acc: 0.7755 - val_loss: 0.9181 - val_acc: 0.7650\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8461 - acc: 0.7768 - val_loss: 0.9156 - val_acc: 0.7680\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8446 - acc: 0.7775 - val_loss: 0.9116 - val_acc: 0.7670\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8444 - acc: 0.7759 - val_loss: 0.9145 - val_acc: 0.7670\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8457 - acc: 0.7751 - val_loss: 0.9119 - val_acc: 0.7640\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8445 - acc: 0.7725 - val_loss: 0.9074 - val_acc: 0.7620\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8435 - acc: 0.7756 - val_loss: 0.9099 - val_acc: 0.7660\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8428 - acc: 0.7761 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8441 - acc: 0.7749 - val_loss: 0.9110 - val_acc: 0.7600\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8440 - acc: 0.7761 - val_loss: 0.9082 - val_acc: 0.7650\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8419 - acc: 0.7771 - val_loss: 0.9155 - val_acc: 0.7670\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8430 - acc: 0.7757 - val_loss: 0.9123 - val_acc: 0.7650\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8449 - acc: 0.7740 - val_loss: 0.9150 - val_acc: 0.7580\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8436 - acc: 0.7752 - val_loss: 0.9153 - val_acc: 0.7720\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8424 - acc: 0.7767 - val_loss: 0.9089 - val_acc: 0.7660\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8417 - acc: 0.7756 - val_loss: 0.9078 - val_acc: 0.7640\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8424 - acc: 0.7765 - val_loss: 0.9113 - val_acc: 0.7690\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8422 - acc: 0.7751 - val_loss: 0.9249 - val_acc: 0.7630\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8417 - acc: 0.7777 - val_loss: 0.9176 - val_acc: 0.7590\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8409 - acc: 0.7759 - val_loss: 0.9201 - val_acc: 0.7570\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8430 - acc: 0.7761 - val_loss: 0.9097 - val_acc: 0.7610\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8405 - acc: 0.7773 - val_loss: 0.9103 - val_acc: 0.7720\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.7756 - val_loss: 0.9056 - val_acc: 0.7650\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.7759 - val_loss: 0.9179 - val_acc: 0.7620\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8403 - acc: 0.7756 - val_loss: 0.9165 - val_acc: 0.7600\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.7764 - val_loss: 0.9082 - val_acc: 0.7630\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8408 - acc: 0.7765 - val_loss: 0.9163 - val_acc: 0.7600\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8409 - acc: 0.7760 - val_loss: 0.9833 - val_acc: 0.7490\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8442 - acc: 0.7756 - val_loss: 0.9144 - val_acc: 0.7620\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8404 - acc: 0.7753 - val_loss: 0.9095 - val_acc: 0.7690\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8387 - acc: 0.7769 - val_loss: 0.9565 - val_acc: 0.7560\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.7767 - val_loss: 0.9085 - val_acc: 0.7620\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8390 - acc: 0.7772 - val_loss: 0.9108 - val_acc: 0.7620\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8384 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7630\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8384 - acc: 0.7764 - val_loss: 0.9125 - val_acc: 0.7620\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8390 - acc: 0.7767 - val_loss: 0.9280 - val_acc: 0.7610\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8387 - acc: 0.7780 - val_loss: 0.9093 - val_acc: 0.7670\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8380 - acc: 0.7784 - val_loss: 0.9485 - val_acc: 0.7590\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8398 - acc: 0.7768 - val_loss: 0.9248 - val_acc: 0.7610\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8386 - acc: 0.7761 - val_loss: 0.9131 - val_acc: 0.7640\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8381 - acc: 0.7772 - val_loss: 0.9120 - val_acc: 0.7710\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8379 - acc: 0.7769 - val_loss: 0.9121 - val_acc: 0.7690\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8381 - acc: 0.7776 - val_loss: 0.9090 - val_acc: 0.7640\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8367 - acc: 0.7795 - val_loss: 0.9198 - val_acc: 0.7630\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8378 - acc: 0.7748 - val_loss: 0.9179 - val_acc: 0.7680\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8359 - acc: 0.7791 - val_loss: 0.9125 - val_acc: 0.7670\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8387 - acc: 0.7755 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8366 - acc: 0.7776 - val_loss: 0.9062 - val_acc: 0.7630\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8371 - acc: 0.7780 - val_loss: 0.9197 - val_acc: 0.7590\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8368 - acc: 0.7768 - val_loss: 0.9163 - val_acc: 0.7620\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8379 - acc: 0.7775 - val_loss: 0.9318 - val_acc: 0.7550\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8366 - acc: 0.7787 - val_loss: 0.9199 - val_acc: 0.7620\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8351 - acc: 0.7795 - val_loss: 0.9142 - val_acc: 0.7680\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.7781 - val_loss: 0.9231 - val_acc: 0.7590\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8357 - acc: 0.7793 - val_loss: 0.9173 - val_acc: 0.7610\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8366 - acc: 0.7785 - val_loss: 0.9114 - val_acc: 0.7620\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8359 - acc: 0.7775 - val_loss: 0.9173 - val_acc: 0.7630\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8351 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7590\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8372 - acc: 0.7772 - val_loss: 0.9154 - val_acc: 0.7570\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8344 - acc: 0.7783 - val_loss: 0.9265 - val_acc: 0.7540\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8359 - acc: 0.7776 - val_loss: 0.9091 - val_acc: 0.7670\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8320 - acc: 0.7796 - val_loss: 0.9200 - val_acc: 0.7620\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8342 - acc: 0.7797 - val_loss: 0.9161 - val_acc: 0.7650\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8346 - acc: 0.7791 - val_loss: 0.9052 - val_acc: 0.7650\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8337 - acc: 0.7816 - val_loss: 0.9069 - val_acc: 0.7670\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8339 - acc: 0.7795 - val_loss: 0.9127 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.7800 - val_loss: 0.9122 - val_acc: 0.7610\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.7812 - val_loss: 0.9122 - val_acc: 0.7680\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8324 - acc: 0.7799 - val_loss: 0.9442 - val_acc: 0.7550\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8343 - acc: 0.7779 - val_loss: 0.9093 - val_acc: 0.7680\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8326 - acc: 0.7823 - val_loss: 0.9219 - val_acc: 0.7610\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8330 - acc: 0.7788 - val_loss: 0.9072 - val_acc: 0.7670\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8319 - acc: 0.7785 - val_loss: 0.9059 - val_acc: 0.7690\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.7789 - val_loss: 0.9032 - val_acc: 0.7690\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8315 - acc: 0.7800 - val_loss: 0.9097 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8311 - acc: 0.7813 - val_loss: 0.9086 - val_acc: 0.7690\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8306 - acc: 0.7788 - val_loss: 0.9259 - val_acc: 0.7640\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8311 - acc: 0.7792 - val_loss: 0.9065 - val_acc: 0.7680\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.7789 - val_loss: 0.9104 - val_acc: 0.7640\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8309 - acc: 0.7799 - val_loss: 0.9115 - val_acc: 0.7630\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8299 - acc: 0.7809 - val_loss: 0.9174 - val_acc: 0.7650\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8344 - acc: 0.7799 - val_loss: 0.9045 - val_acc: 0.7700\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8325 - acc: 0.7803 - val_loss: 0.9127 - val_acc: 0.7720\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8303 - acc: 0.7779 - val_loss: 0.9228 - val_acc: 0.7540\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8300 - acc: 0.7799 - val_loss: 0.9122 - val_acc: 0.7640\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8299 - acc: 0.7801 - val_loss: 0.9072 - val_acc: 0.7650\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8295 - acc: 0.7811 - val_loss: 0.9051 - val_acc: 0.7690\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8314 - acc: 0.7801 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8303 - acc: 0.7820 - val_loss: 0.9182 - val_acc: 0.7610\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8319 - acc: 0.7836 - val_loss: 0.9069 - val_acc: 0.7680\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8306 - acc: 0.7813 - val_loss: 0.9112 - val_acc: 0.7620\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.7823 - val_loss: 0.9062 - val_acc: 0.7690\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8289 - acc: 0.7819 - val_loss: 0.9053 - val_acc: 0.7720\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8286 - acc: 0.7811 - val_loss: 0.9151 - val_acc: 0.7730\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8301 - acc: 0.7805 - val_loss: 0.9287 - val_acc: 0.7660\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8286 - acc: 0.7825 - val_loss: 0.9044 - val_acc: 0.7690\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8268 - acc: 0.7820 - val_loss: 0.9127 - val_acc: 0.7680\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8302 - acc: 0.7781 - val_loss: 0.9099 - val_acc: 0.7680\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8273 - acc: 0.7808 - val_loss: 0.9081 - val_acc: 0.7660\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8282 - acc: 0.7823 - val_loss: 0.9063 - val_acc: 0.7710\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8271 - acc: 0.7829 - val_loss: 0.9027 - val_acc: 0.7630\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8263 - acc: 0.7831 - val_loss: 0.9122 - val_acc: 0.7620\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8277 - acc: 0.7820 - val_loss: 0.9048 - val_acc: 0.7720\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8273 - acc: 0.7831 - val_loss: 0.9161 - val_acc: 0.7600\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8261 - acc: 0.7832 - val_loss: 0.9043 - val_acc: 0.7660\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8268 - acc: 0.7832 - val_loss: 0.9059 - val_acc: 0.7700\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8249 - acc: 0.7839 - val_loss: 0.9073 - val_acc: 0.7660\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8264 - acc: 0.7807 - val_loss: 0.9091 - val_acc: 0.7680\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8272 - acc: 0.7817 - val_loss: 0.9164 - val_acc: 0.7580\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8270 - acc: 0.7817 - val_loss: 0.9028 - val_acc: 0.7680\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8270 - acc: 0.7823 - val_loss: 0.9097 - val_acc: 0.7670\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8264 - acc: 0.7817 - val_loss: 0.9174 - val_acc: 0.7580\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8267 - acc: 0.7827 - val_loss: 0.9014 - val_acc: 0.7680\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8255 - acc: 0.7824 - val_loss: 1.0099 - val_acc: 0.7460\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.7815 - val_loss: 0.9129 - val_acc: 0.7620\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8254 - acc: 0.7820 - val_loss: 0.9050 - val_acc: 0.7690\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8257 - acc: 0.7849 - val_loss: 0.9071 - val_acc: 0.7700\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8230 - acc: 0.7819 - val_loss: 0.9084 - val_acc: 0.7680\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8238 - acc: 0.7843 - val_loss: 0.9078 - val_acc: 0.7660\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8245 - acc: 0.7833 - val_loss: 0.9118 - val_acc: 0.7650\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8242 - acc: 0.7821 - val_loss: 0.9092 - val_acc: 0.7640\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8234 - acc: 0.7844 - val_loss: 0.9052 - val_acc: 0.7750\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8255 - acc: 0.7812 - val_loss: 0.9055 - val_acc: 0.7700\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8240 - acc: 0.7823 - val_loss: 0.9206 - val_acc: 0.7640\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.7849 - val_loss: 0.9097 - val_acc: 0.7700\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8234 - acc: 0.7853 - val_loss: 0.9181 - val_acc: 0.7650\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.7828 - val_loss: 0.9095 - val_acc: 0.7710\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8244 - acc: 0.7835 - val_loss: 0.9430 - val_acc: 0.7520\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8254 - acc: 0.7839 - val_loss: 0.9175 - val_acc: 0.7590\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8230 - acc: 0.7817 - val_loss: 0.9055 - val_acc: 0.7670\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8237 - acc: 0.7859 - val_loss: 0.9032 - val_acc: 0.7640\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8233 - acc: 0.7835 - val_loss: 0.9147 - val_acc: 0.7660\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.7832 - val_loss: 0.9202 - val_acc: 0.7570\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.7827 - val_loss: 0.9368 - val_acc: 0.7600\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.7823 - val_loss: 0.9018 - val_acc: 0.7710\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8217 - acc: 0.7857 - val_loss: 0.9190 - val_acc: 0.7660\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8220 - acc: 0.7859 - val_loss: 0.9252 - val_acc: 0.7610\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.7840 - val_loss: 0.9216 - val_acc: 0.7660\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8224 - acc: 0.7847 - val_loss: 0.9404 - val_acc: 0.7640\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8229 - acc: 0.7845 - val_loss: 0.9016 - val_acc: 0.7670\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8223 - acc: 0.7817 - val_loss: 0.9049 - val_acc: 0.7740\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8211 - acc: 0.7829 - val_loss: 0.8994 - val_acc: 0.7710\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8214 - acc: 0.7849 - val_loss: 0.9074 - val_acc: 0.7640\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8229 - acc: 0.7853 - val_loss: 0.9503 - val_acc: 0.7450\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8225 - acc: 0.7868 - val_loss: 0.9173 - val_acc: 0.7590\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8207 - acc: 0.7833 - val_loss: 0.9102 - val_acc: 0.7700\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8217 - acc: 0.7859 - val_loss: 0.9189 - val_acc: 0.7580\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8224 - acc: 0.7835 - val_loss: 0.9201 - val_acc: 0.7620\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8199 - acc: 0.7839 - val_loss: 0.9130 - val_acc: 0.7650\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8209 - acc: 0.7823 - val_loss: 0.9229 - val_acc: 0.7650\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8200 - acc: 0.7876 - val_loss: 0.9023 - val_acc: 0.7680\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8203 - acc: 0.7836 - val_loss: 0.9066 - val_acc: 0.7680\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8199 - acc: 0.7869 - val_loss: 0.9668 - val_acc: 0.7530\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.7852 - val_loss: 0.9143 - val_acc: 0.7660\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8184 - acc: 0.7863 - val_loss: 0.8999 - val_acc: 0.7690\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.7847 - val_loss: 0.9233 - val_acc: 0.7530\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8214 - acc: 0.7836 - val_loss: 0.9040 - val_acc: 0.7700\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.7829 - val_loss: 0.9128 - val_acc: 0.7650\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8194 - acc: 0.7857 - val_loss: 0.9029 - val_acc: 0.7730\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8180 - acc: 0.7860 - val_loss: 0.9051 - val_acc: 0.7670\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8207 - acc: 0.7843 - val_loss: 0.9086 - val_acc: 0.7650\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8200 - acc: 0.7865 - val_loss: 0.9012 - val_acc: 0.7700\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8173 - acc: 0.7861 - val_loss: 0.9062 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8178 - acc: 0.7857 - val_loss: 0.9215 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8186 - acc: 0.7849 - val_loss: 0.9071 - val_acc: 0.7670\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.7868 - val_loss: 0.9067 - val_acc: 0.7640\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8177 - acc: 0.7880 - val_loss: 0.9070 - val_acc: 0.7670\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7863 - val_loss: 0.9066 - val_acc: 0.7680\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8206 - acc: 0.7851 - val_loss: 0.9257 - val_acc: 0.7620\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8196 - acc: 0.7839 - val_loss: 0.9160 - val_acc: 0.7650\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8175 - acc: 0.7859 - val_loss: 0.9065 - val_acc: 0.7680\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8174 - acc: 0.7855 - val_loss: 0.9095 - val_acc: 0.7610\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.7845 - val_loss: 0.9167 - val_acc: 0.7660\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8199 - acc: 0.7861 - val_loss: 0.9097 - val_acc: 0.7630\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8167 - acc: 0.7880 - val_loss: 0.9177 - val_acc: 0.7640\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8189 - acc: 0.7863 - val_loss: 0.9425 - val_acc: 0.7510\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8186 - acc: 0.7836 - val_loss: 0.9005 - val_acc: 0.7710\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8177 - acc: 0.7876 - val_loss: 0.9093 - val_acc: 0.7660\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8213 - acc: 0.7829 - val_loss: 0.9060 - val_acc: 0.7650\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.7877 - val_loss: 0.9072 - val_acc: 0.7670\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8153 - acc: 0.7865 - val_loss: 0.9348 - val_acc: 0.7550\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8183 - acc: 0.7860 - val_loss: 0.9181 - val_acc: 0.7630\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8170 - acc: 0.7871 - val_loss: 0.9063 - val_acc: 0.7640\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8183 - acc: 0.7865 - val_loss: 0.9060 - val_acc: 0.7670\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8153 - acc: 0.7871 - val_loss: 0.9545 - val_acc: 0.7560\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8164 - acc: 0.7860 - val_loss: 0.9034 - val_acc: 0.7670\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8161 - acc: 0.7875 - val_loss: 0.9043 - val_acc: 0.7660\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8174 - acc: 0.7851 - val_loss: 0.9001 - val_acc: 0.7690\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8170 - acc: 0.7844 - val_loss: 0.9028 - val_acc: 0.7690\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8156 - acc: 0.7847 - val_loss: 0.9028 - val_acc: 0.7660\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8145 - acc: 0.7881 - val_loss: 0.9113 - val_acc: 0.7670\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8142 - acc: 0.7856 - val_loss: 0.8983 - val_acc: 0.7710\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8134 - acc: 0.7865 - val_loss: 0.9103 - val_acc: 0.7650\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8144 - acc: 0.7863 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8160 - acc: 0.7880 - val_loss: 0.9227 - val_acc: 0.7590\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8153 - acc: 0.7869 - val_loss: 0.9051 - val_acc: 0.7650\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8148 - acc: 0.7847 - val_loss: 0.8983 - val_acc: 0.7720\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8126 - acc: 0.7884 - val_loss: 0.9926 - val_acc: 0.7360\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8172 - acc: 0.7847 - val_loss: 0.8999 - val_acc: 0.7700\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8131 - acc: 0.7855 - val_loss: 0.9027 - val_acc: 0.7700\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8143 - acc: 0.7871 - val_loss: 0.9194 - val_acc: 0.7620\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8150 - acc: 0.7869 - val_loss: 0.9016 - val_acc: 0.7650\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8139 - acc: 0.7877 - val_loss: 0.9211 - val_acc: 0.7600\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8140 - acc: 0.7864 - val_loss: 0.9082 - val_acc: 0.7650\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8129 - acc: 0.7895 - val_loss: 0.9026 - val_acc: 0.7680\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8140 - acc: 0.7887 - val_loss: 0.9178 - val_acc: 0.7610\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8139 - acc: 0.7884 - val_loss: 0.9200 - val_acc: 0.7670\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8161 - acc: 0.7852 - val_loss: 0.9611 - val_acc: 0.7500\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.7851 - val_loss: 0.9432 - val_acc: 0.7550\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8137 - acc: 0.7871 - val_loss: 0.9082 - val_acc: 0.7630\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8123 - acc: 0.7869 - val_loss: 0.9019 - val_acc: 0.7630\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8118 - acc: 0.7885 - val_loss: 0.9192 - val_acc: 0.7630\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8137 - acc: 0.7888 - val_loss: 0.9104 - val_acc: 0.7660\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8141 - acc: 0.7881 - val_loss: 0.9139 - val_acc: 0.7650\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8119 - acc: 0.7849 - val_loss: 0.9107 - val_acc: 0.7660\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8141 - acc: 0.7876 - val_loss: 0.9115 - val_acc: 0.7650\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8125 - acc: 0.7868 - val_loss: 0.9023 - val_acc: 0.7650\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.7887 - val_loss: 0.9121 - val_acc: 0.7680\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.7859 - val_loss: 0.9087 - val_acc: 0.7670\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8100 - acc: 0.7908 - val_loss: 0.8982 - val_acc: 0.7660\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8108 - acc: 0.7880 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8101 - acc: 0.7888 - val_loss: 0.9301 - val_acc: 0.7520\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8124 - acc: 0.7861 - val_loss: 0.9182 - val_acc: 0.7610\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8105 - acc: 0.7887 - val_loss: 0.8999 - val_acc: 0.7670\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8098 - acc: 0.7895 - val_loss: 0.9195 - val_acc: 0.7680\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8126 - acc: 0.7872 - val_loss: 0.9415 - val_acc: 0.7540\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8132 - acc: 0.7869 - val_loss: 0.9321 - val_acc: 0.7630\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8111 - acc: 0.7925 - val_loss: 0.9166 - val_acc: 0.7640\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8096 - acc: 0.7888 - val_loss: 0.9071 - val_acc: 0.7690\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8107 - acc: 0.7903 - val_loss: 0.9089 - val_acc: 0.7650\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8088 - acc: 0.7891 - val_loss: 0.9083 - val_acc: 0.7680\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8093 - acc: 0.7881 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8091 - acc: 0.7893 - val_loss: 0.9042 - val_acc: 0.7640\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.7880 - val_loss: 0.9275 - val_acc: 0.7560\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8095 - acc: 0.7903 - val_loss: 0.9046 - val_acc: 0.7690\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8085 - acc: 0.7888 - val_loss: 0.9057 - val_acc: 0.7660\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8091 - acc: 0.7888 - val_loss: 0.9018 - val_acc: 0.7690\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8086 - acc: 0.7911 - val_loss: 0.9152 - val_acc: 0.7710\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8104 - acc: 0.7892 - val_loss: 0.9050 - val_acc: 0.7680\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8102 - acc: 0.7871 - val_loss: 0.9435 - val_acc: 0.7570\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8107 - acc: 0.7879 - val_loss: 0.9042 - val_acc: 0.7680\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8080 - acc: 0.7891 - val_loss: 0.9058 - val_acc: 0.7640\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8095 - acc: 0.7907 - val_loss: 0.9063 - val_acc: 0.7650\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8088 - acc: 0.7899 - val_loss: 0.9045 - val_acc: 0.7720\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8078 - acc: 0.7912 - val_loss: 0.9091 - val_acc: 0.7630\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8113 - acc: 0.7883 - val_loss: 0.8997 - val_acc: 0.7680\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8064 - acc: 0.7896 - val_loss: 0.9333 - val_acc: 0.7520\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8086 - acc: 0.7867 - val_loss: 0.9098 - val_acc: 0.7580\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8078 - acc: 0.7888 - val_loss: 0.8963 - val_acc: 0.7670\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8113 - acc: 0.7899 - val_loss: 0.9028 - val_acc: 0.7650\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8077 - acc: 0.7897 - val_loss: 0.9071 - val_acc: 0.7660\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8087 - acc: 0.7865 - val_loss: 0.9011 - val_acc: 0.7680\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8067 - acc: 0.7915 - val_loss: 0.9009 - val_acc: 0.7670\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8073 - acc: 0.7897 - val_loss: 0.9413 - val_acc: 0.7600\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8086 - acc: 0.7897 - val_loss: 0.9707 - val_acc: 0.7550\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8100 - acc: 0.7873 - val_loss: 0.9057 - val_acc: 0.7660\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8061 - acc: 0.7920 - val_loss: 0.9068 - val_acc: 0.7640\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8065 - acc: 0.7869 - val_loss: 0.9144 - val_acc: 0.7580\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8065 - acc: 0.7881 - val_loss: 0.8985 - val_acc: 0.7670\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8093 - acc: 0.7869 - val_loss: 0.9346 - val_acc: 0.7620\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8074 - acc: 0.7927 - val_loss: 0.8983 - val_acc: 0.7680\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8050 - acc: 0.7909 - val_loss: 0.9002 - val_acc: 0.7690\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8068 - acc: 0.7892 - val_loss: 0.9708 - val_acc: 0.7540\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8080 - acc: 0.7901 - val_loss: 0.8989 - val_acc: 0.7670\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8059 - acc: 0.7905 - val_loss: 0.9032 - val_acc: 0.7650\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8042 - acc: 0.7904 - val_loss: 0.8982 - val_acc: 0.7670\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8044 - acc: 0.7908 - val_loss: 0.8978 - val_acc: 0.7690\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8061 - acc: 0.7891 - val_loss: 0.9260 - val_acc: 0.7620\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8062 - acc: 0.7896 - val_loss: 0.9101 - val_acc: 0.7640\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8076 - acc: 0.7916 - val_loss: 0.9015 - val_acc: 0.7660\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8053 - acc: 0.7904 - val_loss: 0.9111 - val_acc: 0.7620\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8054 - acc: 0.7888 - val_loss: 0.9095 - val_acc: 0.7600\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8064 - acc: 0.7905 - val_loss: 1.0160 - val_acc: 0.7220\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8093 - acc: 0.7895 - val_loss: 0.9150 - val_acc: 0.7630\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8061 - acc: 0.7903 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8051 - acc: 0.7909 - val_loss: 0.9024 - val_acc: 0.7660\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8061 - acc: 0.7913 - val_loss: 0.9054 - val_acc: 0.7660\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8032 - acc: 0.7911 - val_loss: 0.9043 - val_acc: 0.7680\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8059 - acc: 0.7911 - val_loss: 0.8985 - val_acc: 0.7690\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8054 - acc: 0.7911 - val_loss: 0.9205 - val_acc: 0.7600\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8031 - acc: 0.7895 - val_loss: 0.9039 - val_acc: 0.7710\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8036 - acc: 0.7887 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8063 - acc: 0.7889 - val_loss: 0.8979 - val_acc: 0.7630\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8028 - acc: 0.7885 - val_loss: 0.9005 - val_acc: 0.7670\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8018 - acc: 0.7932 - val_loss: 0.9344 - val_acc: 0.7580\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8085 - acc: 0.7897 - val_loss: 0.9131 - val_acc: 0.7630\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8041 - acc: 0.7931 - val_loss: 0.9013 - val_acc: 0.7650\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8044 - acc: 0.7908 - val_loss: 0.9270 - val_acc: 0.7590\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8048 - acc: 0.7925 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8031 - acc: 0.7907 - val_loss: 0.9230 - val_acc: 0.7530\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8040 - acc: 0.7919 - val_loss: 0.9010 - val_acc: 0.7640\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8022 - acc: 0.7908 - val_loss: 0.9301 - val_acc: 0.7550\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8066 - acc: 0.7887 - val_loss: 0.9348 - val_acc: 0.7510\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8026 - acc: 0.7908 - val_loss: 0.9103 - val_acc: 0.7640\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8028 - acc: 0.7916 - val_loss: 0.9129 - val_acc: 0.7600\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8013 - acc: 0.7920 - val_loss: 0.9137 - val_acc: 0.7540\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8020 - acc: 0.7913 - val_loss: 1.0339 - val_acc: 0.7370\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8053 - acc: 0.7905 - val_loss: 0.8995 - val_acc: 0.7690\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8023 - acc: 0.7893 - val_loss: 0.8994 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8015 - acc: 0.7936 - val_loss: 0.8994 - val_acc: 0.7670\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8046 - acc: 0.7923 - val_loss: 0.9146 - val_acc: 0.7620\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8037 - acc: 0.7907 - val_loss: 0.9139 - val_acc: 0.7630\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8015 - acc: 0.7923 - val_loss: 0.9190 - val_acc: 0.7650\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8009 - acc: 0.7924 - val_loss: 0.8967 - val_acc: 0.7690\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7979 - acc: 0.7925 - val_loss: 0.9146 - val_acc: 0.7600\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8038 - acc: 0.7916 - val_loss: 0.9045 - val_acc: 0.7650\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8023 - acc: 0.7921 - val_loss: 0.9079 - val_acc: 0.7680\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7997 - acc: 0.7927 - val_loss: 0.9032 - val_acc: 0.7720\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7995 - acc: 0.7928 - val_loss: 0.9060 - val_acc: 0.7620\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7994 - acc: 0.7923 - val_loss: 0.9357 - val_acc: 0.7540\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8028 - acc: 0.7883 - val_loss: 0.9011 - val_acc: 0.7620\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8002 - acc: 0.7947 - val_loss: 0.9180 - val_acc: 0.7620\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8003 - acc: 0.7944 - val_loss: 0.8976 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7986 - acc: 0.7928 - val_loss: 0.9240 - val_acc: 0.7600\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8025 - acc: 0.7921 - val_loss: 0.8977 - val_acc: 0.7690\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8010 - acc: 0.7921 - val_loss: 0.9012 - val_acc: 0.7680\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8006 - acc: 0.7899 - val_loss: 0.8968 - val_acc: 0.7660\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7994 - acc: 0.7935 - val_loss: 0.9531 - val_acc: 0.7510\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8023 - acc: 0.7928 - val_loss: 0.9041 - val_acc: 0.7640\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8022 - acc: 0.7896 - val_loss: 0.8960 - val_acc: 0.7710\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7988 - acc: 0.7919 - val_loss: 0.9243 - val_acc: 0.7590\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7997 - acc: 0.7919 - val_loss: 0.9008 - val_acc: 0.7680\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7998 - acc: 0.7889 - val_loss: 0.9130 - val_acc: 0.7590\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8012 - acc: 0.7931 - val_loss: 0.9297 - val_acc: 0.7580\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7995 - acc: 0.7933 - val_loss: 0.8984 - val_acc: 0.7640\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7986 - acc: 0.7921 - val_loss: 0.9024 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8006 - acc: 0.7929 - val_loss: 0.9042 - val_acc: 0.7680\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step\n",
      "1500/1500 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7956617623964946, 0.7918666666984558]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8423501462936401, 0.7839999996821085]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9918 - acc: 0.1472 - val_loss: 1.9483 - val_acc: 0.1650\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9600 - acc: 0.1623 - val_loss: 1.9348 - val_acc: 0.2020\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9477 - acc: 0.1827 - val_loss: 1.9270 - val_acc: 0.2170\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9404 - acc: 0.1832 - val_loss: 1.9198 - val_acc: 0.2200\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9319 - acc: 0.1941 - val_loss: 1.9135 - val_acc: 0.2270\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9274 - acc: 0.2045 - val_loss: 1.9071 - val_acc: 0.2320\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9222 - acc: 0.2041 - val_loss: 1.9007 - val_acc: 0.2400\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9120 - acc: 0.2185 - val_loss: 1.8930 - val_acc: 0.2540\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8996 - acc: 0.2352 - val_loss: 1.8842 - val_acc: 0.2600\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8966 - acc: 0.2307 - val_loss: 1.8747 - val_acc: 0.2630\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8860 - acc: 0.2497 - val_loss: 1.8642 - val_acc: 0.2730\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8802 - acc: 0.2471 - val_loss: 1.8534 - val_acc: 0.2760\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8716 - acc: 0.2473 - val_loss: 1.8413 - val_acc: 0.2950\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8693 - acc: 0.2487 - val_loss: 1.8287 - val_acc: 0.3070\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8571 - acc: 0.2656 - val_loss: 1.8137 - val_acc: 0.3230\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8364 - acc: 0.2799 - val_loss: 1.7958 - val_acc: 0.3450\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8223 - acc: 0.2941 - val_loss: 1.7758 - val_acc: 0.3580\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8097 - acc: 0.2965 - val_loss: 1.7540 - val_acc: 0.3660\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7957 - acc: 0.3040 - val_loss: 1.7305 - val_acc: 0.3940\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7752 - acc: 0.3169 - val_loss: 1.7050 - val_acc: 0.4130\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7601 - acc: 0.3192 - val_loss: 1.6797 - val_acc: 0.4340\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7412 - acc: 0.3356 - val_loss: 1.6525 - val_acc: 0.4600\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7245 - acc: 0.3367 - val_loss: 1.6234 - val_acc: 0.4780\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7042 - acc: 0.3563 - val_loss: 1.5951 - val_acc: 0.5030\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6870 - acc: 0.3633 - val_loss: 1.5667 - val_acc: 0.5130\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6679 - acc: 0.3651 - val_loss: 1.5383 - val_acc: 0.5290\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6470 - acc: 0.3845 - val_loss: 1.5119 - val_acc: 0.5490\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6289 - acc: 0.3887 - val_loss: 1.4834 - val_acc: 0.5630\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6062 - acc: 0.3945 - val_loss: 1.4530 - val_acc: 0.5660\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5881 - acc: 0.4055 - val_loss: 1.4278 - val_acc: 0.5830\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5736 - acc: 0.4147 - val_loss: 1.4020 - val_acc: 0.5900\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5427 - acc: 0.4297 - val_loss: 1.3733 - val_acc: 0.6050\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5371 - acc: 0.4236 - val_loss: 1.3478 - val_acc: 0.6120\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5169 - acc: 0.4305 - val_loss: 1.3240 - val_acc: 0.6270\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5020 - acc: 0.4316 - val_loss: 1.3028 - val_acc: 0.6350\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4773 - acc: 0.4568 - val_loss: 1.2781 - val_acc: 0.6360\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4553 - acc: 0.4597 - val_loss: 1.2538 - val_acc: 0.6430\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4554 - acc: 0.4603 - val_loss: 1.2366 - val_acc: 0.6460\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4393 - acc: 0.4657 - val_loss: 1.2180 - val_acc: 0.6480\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4143 - acc: 0.4719 - val_loss: 1.1967 - val_acc: 0.6550\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4092 - acc: 0.4756 - val_loss: 1.1811 - val_acc: 0.6570\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3861 - acc: 0.4873 - val_loss: 1.1595 - val_acc: 0.6600\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3839 - acc: 0.4887 - val_loss: 1.1449 - val_acc: 0.6580\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3593 - acc: 0.4969 - val_loss: 1.1260 - val_acc: 0.6650\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3581 - acc: 0.4936 - val_loss: 1.1136 - val_acc: 0.6650\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3476 - acc: 0.4945 - val_loss: 1.1020 - val_acc: 0.6650\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3220 - acc: 0.5040 - val_loss: 1.0841 - val_acc: 0.6670\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3243 - acc: 0.5075 - val_loss: 1.0757 - val_acc: 0.6700\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3148 - acc: 0.5080 - val_loss: 1.0608 - val_acc: 0.6730\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3071 - acc: 0.5099 - val_loss: 1.0510 - val_acc: 0.6740\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2893 - acc: 0.5201 - val_loss: 1.0425 - val_acc: 0.6760\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2769 - acc: 0.5061 - val_loss: 1.0267 - val_acc: 0.6820\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2582 - acc: 0.5305 - val_loss: 1.0147 - val_acc: 0.6800\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2350 - acc: 0.5411 - val_loss: 0.9988 - val_acc: 0.6840\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2506 - acc: 0.5373 - val_loss: 0.9891 - val_acc: 0.6880\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2358 - acc: 0.5349 - val_loss: 0.9820 - val_acc: 0.6880\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2162 - acc: 0.5425 - val_loss: 0.9681 - val_acc: 0.6930\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2143 - acc: 0.5453 - val_loss: 0.9629 - val_acc: 0.6910\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2182 - acc: 0.5464 - val_loss: 0.9565 - val_acc: 0.6960\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2084 - acc: 0.5492 - val_loss: 0.9481 - val_acc: 0.6970\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1818 - acc: 0.5573 - val_loss: 0.9377 - val_acc: 0.6970\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1726 - acc: 0.5603 - val_loss: 0.9267 - val_acc: 0.7020\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1791 - acc: 0.5505 - val_loss: 0.9204 - val_acc: 0.7070\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1781 - acc: 0.5557 - val_loss: 0.9163 - val_acc: 0.7050\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1593 - acc: 0.5609 - val_loss: 0.9068 - val_acc: 0.7070\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1605 - acc: 0.5668 - val_loss: 0.9005 - val_acc: 0.7100\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1482 - acc: 0.5655 - val_loss: 0.8930 - val_acc: 0.7110\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1351 - acc: 0.5756 - val_loss: 0.8856 - val_acc: 0.7100\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1320 - acc: 0.5781 - val_loss: 0.8789 - val_acc: 0.7140\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1325 - acc: 0.5777 - val_loss: 0.8746 - val_acc: 0.7140\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1147 - acc: 0.5903 - val_loss: 0.8680 - val_acc: 0.7210\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1039 - acc: 0.5848 - val_loss: 0.8613 - val_acc: 0.7150\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1062 - acc: 0.5868 - val_loss: 0.8574 - val_acc: 0.7200\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1019 - acc: 0.5920 - val_loss: 0.8533 - val_acc: 0.7210\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0954 - acc: 0.5860 - val_loss: 0.8496 - val_acc: 0.7250\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0834 - acc: 0.5956 - val_loss: 0.8426 - val_acc: 0.7240\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0755 - acc: 0.5940 - val_loss: 0.8366 - val_acc: 0.7250\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1008 - acc: 0.5912 - val_loss: 0.8334 - val_acc: 0.7250\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0771 - acc: 0.5977 - val_loss: 0.8269 - val_acc: 0.7280\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0673 - acc: 0.6096 - val_loss: 0.8209 - val_acc: 0.7270\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0548 - acc: 0.6040 - val_loss: 0.8150 - val_acc: 0.7280\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0561 - acc: 0.6091 - val_loss: 0.8129 - val_acc: 0.7290\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0383 - acc: 0.6152 - val_loss: 0.8054 - val_acc: 0.7320\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0466 - acc: 0.6100 - val_loss: 0.8032 - val_acc: 0.7310\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0562 - acc: 0.6013 - val_loss: 0.7992 - val_acc: 0.7320\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0338 - acc: 0.6183 - val_loss: 0.7978 - val_acc: 0.7310\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0136 - acc: 0.6164 - val_loss: 0.7884 - val_acc: 0.7340\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0180 - acc: 0.6269 - val_loss: 0.7860 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0262 - acc: 0.6245 - val_loss: 0.7846 - val_acc: 0.7400\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0130 - acc: 0.6165 - val_loss: 0.7809 - val_acc: 0.7400\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0281 - acc: 0.6172 - val_loss: 0.7798 - val_acc: 0.7380\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0070 - acc: 0.6343 - val_loss: 0.7751 - val_acc: 0.7470\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0012 - acc: 0.6295 - val_loss: 0.7704 - val_acc: 0.7400\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0100 - acc: 0.6233 - val_loss: 0.7706 - val_acc: 0.7400\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9959 - acc: 0.6311 - val_loss: 0.7647 - val_acc: 0.7440\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9863 - acc: 0.6355 - val_loss: 0.7591 - val_acc: 0.7480\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9847 - acc: 0.6289 - val_loss: 0.7585 - val_acc: 0.7470\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9823 - acc: 0.6299 - val_loss: 0.7556 - val_acc: 0.7490\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9677 - acc: 0.6449 - val_loss: 0.7498 - val_acc: 0.7470\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9724 - acc: 0.6417 - val_loss: 0.7471 - val_acc: 0.7480\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9709 - acc: 0.6439 - val_loss: 0.7448 - val_acc: 0.7510\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9685 - acc: 0.6391 - val_loss: 0.7423 - val_acc: 0.7490\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9650 - acc: 0.6392 - val_loss: 0.7406 - val_acc: 0.7480\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9491 - acc: 0.6495 - val_loss: 0.7372 - val_acc: 0.7520\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9551 - acc: 0.6467 - val_loss: 0.7324 - val_acc: 0.7540\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9553 - acc: 0.6505 - val_loss: 0.7321 - val_acc: 0.7550\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9473 - acc: 0.6452 - val_loss: 0.7288 - val_acc: 0.7560\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9539 - acc: 0.6484 - val_loss: 0.7276 - val_acc: 0.7560\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9387 - acc: 0.6496 - val_loss: 0.7231 - val_acc: 0.7570\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9373 - acc: 0.6469 - val_loss: 0.7205 - val_acc: 0.7600\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9302 - acc: 0.6528 - val_loss: 0.7208 - val_acc: 0.7570\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9178 - acc: 0.6584 - val_loss: 0.7170 - val_acc: 0.7590\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9326 - acc: 0.6593 - val_loss: 0.7170 - val_acc: 0.7560\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9225 - acc: 0.6527 - val_loss: 0.7148 - val_acc: 0.7590\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9221 - acc: 0.6581 - val_loss: 0.7124 - val_acc: 0.7630\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9223 - acc: 0.6503 - val_loss: 0.7132 - val_acc: 0.7620\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9115 - acc: 0.6611 - val_loss: 0.7085 - val_acc: 0.7620\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9135 - acc: 0.6653 - val_loss: 0.7072 - val_acc: 0.7610\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9200 - acc: 0.6525 - val_loss: 0.7075 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8994 - acc: 0.6651 - val_loss: 0.7054 - val_acc: 0.7590\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8993 - acc: 0.6657 - val_loss: 0.7010 - val_acc: 0.7600\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9084 - acc: 0.6592 - val_loss: 0.7003 - val_acc: 0.7610\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9094 - acc: 0.6616 - val_loss: 0.7005 - val_acc: 0.7620\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8935 - acc: 0.6693 - val_loss: 0.6976 - val_acc: 0.7610\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8889 - acc: 0.6721 - val_loss: 0.6957 - val_acc: 0.7630\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8940 - acc: 0.6637 - val_loss: 0.6940 - val_acc: 0.7640\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8710 - acc: 0.6696 - val_loss: 0.6912 - val_acc: 0.7650\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8788 - acc: 0.6773 - val_loss: 0.6881 - val_acc: 0.7630\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8760 - acc: 0.6715 - val_loss: 0.6879 - val_acc: 0.7610\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8871 - acc: 0.6712 - val_loss: 0.6851 - val_acc: 0.7580\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8684 - acc: 0.6772 - val_loss: 0.6837 - val_acc: 0.7640\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8809 - acc: 0.6728 - val_loss: 0.6853 - val_acc: 0.7570\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8913 - acc: 0.6704 - val_loss: 0.6828 - val_acc: 0.7600\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8465 - acc: 0.6845 - val_loss: 0.6813 - val_acc: 0.7630\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8423 - acc: 0.6837 - val_loss: 0.6788 - val_acc: 0.7640\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8602 - acc: 0.6835 - val_loss: 0.6775 - val_acc: 0.7620\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8544 - acc: 0.6759 - val_loss: 0.6755 - val_acc: 0.7590\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8519 - acc: 0.6845 - val_loss: 0.6730 - val_acc: 0.7680\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8529 - acc: 0.6797 - val_loss: 0.6719 - val_acc: 0.7610\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8556 - acc: 0.6851 - val_loss: 0.6714 - val_acc: 0.7670\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8460 - acc: 0.6848 - val_loss: 0.6709 - val_acc: 0.7610\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8493 - acc: 0.6816 - val_loss: 0.6692 - val_acc: 0.7640\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8433 - acc: 0.6907 - val_loss: 0.6681 - val_acc: 0.7680\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8390 - acc: 0.6824 - val_loss: 0.6684 - val_acc: 0.7630\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8280 - acc: 0.6952 - val_loss: 0.6655 - val_acc: 0.7600\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8275 - acc: 0.6871 - val_loss: 0.6623 - val_acc: 0.7680\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8408 - acc: 0.6847 - val_loss: 0.6632 - val_acc: 0.7640\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8237 - acc: 0.6871 - val_loss: 0.6615 - val_acc: 0.7640\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8387 - acc: 0.6832 - val_loss: 0.6624 - val_acc: 0.7650\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8247 - acc: 0.6865 - val_loss: 0.6597 - val_acc: 0.7650\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8364 - acc: 0.6913 - val_loss: 0.6598 - val_acc: 0.7660\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8183 - acc: 0.6921 - val_loss: 0.6581 - val_acc: 0.7620\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8186 - acc: 0.6924 - val_loss: 0.6554 - val_acc: 0.7590\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8191 - acc: 0.6951 - val_loss: 0.6547 - val_acc: 0.7610\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8220 - acc: 0.6927 - val_loss: 0.6530 - val_acc: 0.7650\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8197 - acc: 0.6951 - val_loss: 0.6506 - val_acc: 0.7650\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8072 - acc: 0.6969 - val_loss: 0.6506 - val_acc: 0.7630\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8138 - acc: 0.6928 - val_loss: 0.6512 - val_acc: 0.7620\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8063 - acc: 0.6996 - val_loss: 0.6501 - val_acc: 0.7640\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8054 - acc: 0.7004 - val_loss: 0.6507 - val_acc: 0.7620\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8194 - acc: 0.6917 - val_loss: 0.6492 - val_acc: 0.7640\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8160 - acc: 0.6948 - val_loss: 0.6503 - val_acc: 0.7650\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8005 - acc: 0.7029 - val_loss: 0.6489 - val_acc: 0.7620\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8009 - acc: 0.7023 - val_loss: 0.6484 - val_acc: 0.7620\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8033 - acc: 0.6963 - val_loss: 0.6449 - val_acc: 0.7620\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7995 - acc: 0.6981 - val_loss: 0.6450 - val_acc: 0.7630\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7977 - acc: 0.7012 - val_loss: 0.6436 - val_acc: 0.7630\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7915 - acc: 0.6991 - val_loss: 0.6436 - val_acc: 0.7630\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7980 - acc: 0.7013 - val_loss: 0.6429 - val_acc: 0.7640\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7802 - acc: 0.7127 - val_loss: 0.6391 - val_acc: 0.7660\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7880 - acc: 0.7069 - val_loss: 0.6401 - val_acc: 0.7670\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7847 - acc: 0.7059 - val_loss: 0.6399 - val_acc: 0.7660\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7824 - acc: 0.7064 - val_loss: 0.6387 - val_acc: 0.7690\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7795 - acc: 0.7103 - val_loss: 0.6392 - val_acc: 0.7690\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7791 - acc: 0.7133 - val_loss: 0.6384 - val_acc: 0.7660\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7723 - acc: 0.7136 - val_loss: 0.6357 - val_acc: 0.7640\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7902 - acc: 0.7041 - val_loss: 0.6379 - val_acc: 0.7660\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7720 - acc: 0.7108 - val_loss: 0.6357 - val_acc: 0.7680\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7851 - acc: 0.7048 - val_loss: 0.6372 - val_acc: 0.7660\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7778 - acc: 0.7089 - val_loss: 0.6357 - val_acc: 0.7660\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7526 - acc: 0.7153 - val_loss: 0.6350 - val_acc: 0.7660\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7728 - acc: 0.7128 - val_loss: 0.6325 - val_acc: 0.7660\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7815 - acc: 0.7181 - val_loss: 0.6341 - val_acc: 0.7680\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7592 - acc: 0.7163 - val_loss: 0.6340 - val_acc: 0.7670\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7749 - acc: 0.7039 - val_loss: 0.6336 - val_acc: 0.7680\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7738 - acc: 0.7045 - val_loss: 0.6330 - val_acc: 0.7660\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7549 - acc: 0.7213 - val_loss: 0.6307 - val_acc: 0.7670\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7708 - acc: 0.7125 - val_loss: 0.6315 - val_acc: 0.7670\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7665 - acc: 0.7112 - val_loss: 0.6324 - val_acc: 0.7710\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7619 - acc: 0.7099 - val_loss: 0.6289 - val_acc: 0.7690\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7522 - acc: 0.7156 - val_loss: 0.6279 - val_acc: 0.7680\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7422 - acc: 0.7240 - val_loss: 0.6267 - val_acc: 0.7670\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7519 - acc: 0.7189 - val_loss: 0.6257 - val_acc: 0.7730\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7449 - acc: 0.7208 - val_loss: 0.6254 - val_acc: 0.7750\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7354 - acc: 0.7249 - val_loss: 0.6254 - val_acc: 0.7740\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7614 - acc: 0.7161 - val_loss: 0.6261 - val_acc: 0.7710\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7609 - acc: 0.7125 - val_loss: 0.6247 - val_acc: 0.7720\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7501 - acc: 0.7167 - val_loss: 0.6232 - val_acc: 0.7720\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7417 - acc: 0.7176 - val_loss: 0.6230 - val_acc: 0.7710\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7429 - acc: 0.7204 - val_loss: 0.6208 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 25us/step\n",
      "4000/4000 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7154712603019946, 0.15745454545454546]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6870732326507567, 0.1575]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
